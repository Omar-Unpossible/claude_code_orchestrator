"""Tests for OutputMonitor component.

This module tests the output monitoring functionality including:
- Completion detection with multiple heuristics
- Error pattern matching
- Rate limit detection
- Buffer management and overflow handling
- Observer pattern notifications
- Real-time monitoring
- Thread safety

Tests cover all functionality specified in plans/02_interfaces.json deliverable 2.3.
"""

import pytest
import time
import threading
from collections import deque
from unittest.mock import Mock, MagicMock, patch
from io import StringIO

from src.agents.output_monitor import OutputMonitor
from src.core.exceptions import MonitoringException


class TestOutputMonitorInitialization:
    """Test initialization and configuration."""

    def test_initialization_default_config(self):
        """Test initialization with default configuration."""
        monitor = OutputMonitor()

        assert isinstance(monitor._buffer, deque)
        assert monitor._observers == []
        assert not monitor.is_monitoring
        assert monitor._completion_event is not None

    def test_initialization_custom_buffer_size(self):
        """Test initialization with custom buffer size."""
        monitor = OutputMonitor(buffer_size=5000)

        assert monitor._buffer.maxlen == 5000

    def test_initialization_custom_completion_markers(self):
        """Test initialization with custom completion markers."""
        custom_markers = ['DONE', 'COMPLETE']
        monitor = OutputMonitor(completion_markers=custom_markers)

        assert 'DONE' in monitor.completion_markers
        assert 'COMPLETE' in monitor.completion_markers

    def test_initialization_custom_error_patterns(self):
        """Test initialization with custom error patterns."""
        custom_errors = ['FATAL:', 'CRASH:']
        monitor = OutputMonitor(error_markers=custom_errors)

        assert 'FATAL:' in monitor.error_markers
        assert 'CRASH:' in monitor.error_markers

    def test_initialization_custom_rate_limit_patterns(self):
        """Test initialization with custom rate limit patterns."""
        custom_limits = ['quota exceeded', 'throttled']
        monitor = OutputMonitor(rate_limit_markers=custom_limits)

        assert 'quota exceeded' in monitor.rate_limit_markers
        assert 'throttled' in monitor.rate_limit_markers

    def test_initialization_with_output_file(self, tmp_path):
        """Test initialization with output file logging."""
        output_file = tmp_path / "output.log"
        monitor = OutputMonitor(output_file=output_file)

        assert monitor.output_file == output_file


class TestCompletionDetection:
    """Test completion detection with various patterns."""

    @pytest.fixture
    def monitor(self):
        """Create monitor instance for testing."""
        mon = OutputMonitor(completion_timeout=0.2)
        yield mon
        # Safety cleanup: stop monitoring if still active
        if mon.is_monitoring:
            mon.stop_monitoring()

    def test_completion_marker_ready_for_next(self, monitor, fast_time):
        """Test detection of 'Ready for next' marker."""
        monitor._process_line("Processing data")
        monitor._process_line("Completed task")
        monitor._process_line("Ready for next")

        # Wait for timeout after marker
        time.sleep(0.3)

        assert monitor.is_complete()

    def test_completion_marker_triple_chevron(self, monitor, fast_time):
        """Test detection of '>>>' marker."""
        monitor._process_line("Output here")
        monitor._process_line(">>>")

        time.sleep(0.3)

        assert monitor.is_complete()

    def test_completion_marker_command_completed(self, monitor, fast_time):
        """Test detection of 'Command completed' marker."""
        monitor._process_line("Executing...")
        monitor._process_line("Command completed")

        time.sleep(0.3)

        assert monitor.is_complete()

    def test_completion_marker_finished(self, monitor, fast_time):
        """Test detection of 'Finished' marker."""
        monitor._process_line("Processing...")
        monitor._process_line("Finished successfully")

        time.sleep(0.3)

        assert monitor.is_complete()

    def test_no_completion_marker(self, monitor):
        """Test that incomplete output is not marked complete."""
        monitor._process_line("Still processing...")
        monitor._process_line("Working on it...")

        assert not monitor.is_complete()

    def test_completion_requires_timeout(self, monitor, fast_time):
        """Test that completion requires timeout after marker."""
        monitor._process_line("Done")
        monitor._process_line("Ready for next")

        # Without timeout, should not be complete
        assert not monitor.is_complete()

        # After timeout, should be complete
        time.sleep(0.3)
        assert monitor.is_complete()

    def test_completion_marker_case_insensitive(self, monitor, fast_time):
        """Test that completion markers are case-insensitive."""
        monitor._process_line("READY FOR NEXT")

        time.sleep(0.3)

        assert monitor.is_complete()

    def test_completion_marker_partial_match(self, monitor, fast_time):
        """Test that partial marker matches work."""
        monitor._process_line("Task is ready for next operation")

        time.sleep(0.3)

        assert monitor.is_complete()

    def test_multiple_completion_markers(self, monitor, fast_time):
        """Test handling of multiple completion markers."""
        monitor._process_line("Command completed")
        # Need to wait for timeout after first marker
        time.sleep(0.1)
        monitor._process_line("Ready for next")

        time.sleep(0.3)

        assert monitor.is_complete()

    def test_completion_reset_after_clear(self, monitor, fast_time):
        """Test that clearing buffer resets completion detection."""
        # First completion
        monitor._process_line("Ready for next")
        time.sleep(0.3)
        assert monitor.is_complete()

        # Clear and add new output
        monitor.clear_buffer()
        monitor._completion_event.clear()
        monitor._completion_marker_found = False
        monitor._process_line("New processing...")

        # Should not be complete anymore
        assert not monitor.is_complete()


class TestErrorDetection:
    """Test error pattern matching."""

    @pytest.fixture
    def monitor(self):
        """Create monitor instance for testing."""
        mon = OutputMonitor()
        yield mon
        # Safety cleanup: stop monitoring if still active
        if mon.is_monitoring:
            mon.stop_monitoring()

    def test_error_detection_error_prefix(self, monitor):
        """Test detection of 'Error:' prefix."""
        monitor._process_line("Executing command")
        monitor._process_line("Error: File not found")

        error = monitor.detect_error()
        assert error is not None
        assert 'Error' in error or 'File not found' in error

    def test_error_detection_exception_prefix(self, monitor):
        """Test detection of 'Exception:' prefix."""
        monitor._process_line("Processing")
        monitor._process_line("Exception: ValueError occurred")

        error = monitor.detect_error()
        assert error is not None
        assert 'Exception' in error or 'ValueError' in error

    def test_error_detection_traceback(self, monitor):
        """Test detection of 'Traceback' pattern."""
        monitor._process_line("Running code")
        # The marker is "Traceback:" so we need output with "Traceback:" in it
        monitor._process_line("Traceback: error in module")
        monitor._process_line("  File test.py")

        error = monitor.detect_error()
        # Traceback pattern matches case-insensitively
        assert error is not None
        assert 'traceback' in error.lower()

    def test_error_detection_failed_keyword(self, monitor):
        """Test detection of 'FAILED' keyword."""
        monitor._process_line("Test suite FAILED: 3 tests failed")

        error = monitor.detect_error()
        assert error is not None
        assert 'FAILED' in error

    def test_error_detection_assertion_error(self, monitor):
        """Test detection of 'AssertionError' pattern."""
        monitor._process_line("Test running")
        monitor._process_line("AssertionError: expected True, got False")

        error = monitor.detect_error()
        assert error is not None
        assert 'AssertionError' in error

    def test_no_error_detection(self, monitor):
        """Test that normal output has no errors."""
        monitor._process_line("Processing successfully")
        monitor._process_line("All tests passed")

        error = monitor.detect_error()
        assert error is None

    def test_error_detection_multiple_errors(self, monitor):
        """Test detection with multiple error lines."""
        monitor._process_line("Error: First error")
        monitor._process_line("Warning: Something")
        monitor._process_line("Error: Second error")

        error = monitor.detect_error()
        assert error is not None
        # Should contain at least one error
        assert 'Error' in error or 'error' in error

    def test_error_detection_case_insensitive(self, monitor):
        """Test that error detection is case-insensitive."""
        monitor._process_line("error: lowercase error message")

        # Should still detect (patterns are case-insensitive)
        error = monitor.detect_error()
        assert error is not None


class TestRateLimitDetection:
    """Test rate limit pattern matching."""

    @pytest.fixture
    def monitor(self):
        """Create monitor instance for testing."""
        mon = OutputMonitor()
        yield mon
        # Safety cleanup: stop monitoring if still active
        if mon.is_monitoring:
            mon.stop_monitoring()

    def test_rate_limit_detection_rate_limit(self, monitor):
        """Test detection of 'rate limit' pattern."""
        monitor._process_line("API call failed")
        monitor._process_line("rate limit exceeded, please try again later")

        assert monitor.detect_rate_limit()

    def test_rate_limit_detection_too_many_requests(self, monitor):
        """Test detection of 'too many requests' pattern."""
        monitor._process_line("Error 429: too many requests")

        assert monitor.detect_rate_limit()

    def test_rate_limit_detection_try_again(self, monitor):
        """Test detection of 'try again' pattern."""
        monitor._process_line("Request blocked, try again in 60 seconds")

        assert monitor.detect_rate_limit()

    def test_rate_limit_detection_quota_exceeded(self, monitor):
        """Test detection of 'quota exceeded' pattern."""
        monitor._process_line("API quota exceeded for this hour")

        assert monitor.detect_rate_limit()

    def test_no_rate_limit_detection(self, monitor):
        """Test that normal output has no rate limits."""
        monitor._process_line("Request successful")
        monitor._process_line("Processing complete")

        assert not monitor.detect_rate_limit()

    def test_rate_limit_detection_case_insensitive(self, monitor):
        """Test that rate limit detection is case-insensitive."""
        monitor._process_line("RATE LIMIT EXCEEDED")

        assert monitor.detect_rate_limit()


class TestBufferManagement:
    """Test buffer overflow handling and management."""

    def test_buffer_size_limit(self):
        """Test that buffer respects size limit."""
        monitor = OutputMonitor(buffer_size=100)

        # Add 150 lines
        for i in range(150):
            monitor._process_line(f"Line {i}")

        # Should only keep last 100
        assert len(monitor._buffer) == 100
        # First line should be "Line 50" (oldest 50 dropped)
        assert "Line 50" in monitor._buffer[0]

    def test_buffer_overflow_drops_oldest(self):
        """Test that buffer drops oldest lines on overflow."""
        monitor = OutputMonitor(buffer_size=10)

        for i in range(20):
            monitor._process_line(f"Line {i}")

        buffer_content = list(monitor._buffer)

        # Should contain lines 10-19
        assert "Line 10" in buffer_content[0]
        assert "Line 19" in buffer_content[-1]
        # Should not contain line 0
        assert not any("Line 0" in line for line in buffer_content)

    def test_get_buffer_all_lines(self):
        """Test getting all buffer lines."""
        monitor = OutputMonitor()

        lines = ["Line 1", "Line 2", "Line 3"]
        for line in lines:
            monitor._process_line(line)

        buffer = monitor.get_buffer(lines=100)  # Get all

        assert len(buffer) == 3
        assert "Line 1" in buffer[0]
        assert "Line 3" in buffer[2]

    def test_get_buffer_limited_lines(self):
        """Test getting limited number of buffer lines."""
        monitor = OutputMonitor()

        for i in range(100):
            monitor._process_line(f"Line {i}")

        buffer = monitor.get_buffer(lines=10)

        assert len(buffer) == 10
        # Should get last 10 lines
        assert "Line 90" in buffer[0]
        assert "Line 99" in buffer[-1]

    def test_clear_buffer(self):
        """Test clearing buffer."""
        monitor = OutputMonitor()

        for i in range(10):
            monitor._process_line(f"Line {i}")

        assert len(monitor._buffer) > 0

        monitor.clear_buffer()

        assert len(monitor._buffer) == 0

    def test_buffer_thread_safety(self):
        """Test buffer operations are thread-safe."""
        monitor = OutputMonitor()
        errors = []

        def add_lines(start, count):
            try:
                for i in range(start, start + count):
                    monitor._process_line(f"Thread line {i}")
                    # Removed sleep to reduce test time
            except Exception as e:
                errors.append(e)

        # Create multiple threads adding to buffer (reduced from 5 to 3)
        threads = []
        for i in range(3):
            # Reduced from 100 to 50 iterations per thread
            thread = threading.Thread(target=add_lines, args=(i * 50, 50))
            threads.append(thread)
            thread.start()

        for thread in threads:
            thread.join(timeout=2.0)  # Add timeout to prevent hanging

        # No errors should occur
        assert len(errors) == 0
        # Buffer should have entries
        assert len(monitor._buffer) > 0


class TestObserverNotification:
    """Test observer pattern implementation."""

    @pytest.fixture
    def monitor(self):
        """Create monitor instance for testing."""
        mon = OutputMonitor()
        yield mon
        # Safety cleanup: stop monitoring if still active
        if mon.is_monitoring:
            mon.stop_monitoring()

    def test_register_observer(self, monitor):
        """Test registering an observer."""
        callback = Mock()

        monitor.register_observer(callback)

        assert callback in monitor._observers

    def test_register_multiple_observers(self, monitor):
        """Test registering multiple observers."""
        callback1 = Mock()
        callback2 = Mock()

        monitor.register_observer(callback1)
        monitor.register_observer(callback2)

        assert len(monitor._observers) == 2

    def test_observer_notification_on_output(self, monitor):
        """Test observers are notified on new output."""
        callback = Mock()
        monitor.register_observer(callback)

        monitor._process_line("New line")

        # Observer should be called with event dict
        callback.assert_called()
        # Check that the call includes the event
        args = callback.call_args_list
        assert len(args) > 0

    def test_observer_notification_on_completion(self, monitor):
        """Test observers are notified on completion."""
        callback = Mock()
        monitor.register_observer(callback)

        monitor._process_line("Ready for next")
        time.sleep(0.3)
        monitor.is_complete()  # Triggers completion notification

        # Observer should be called multiple times
        assert callback.call_count >= 1

    def test_observer_notification_on_error(self, monitor):
        """Test observers are notified on error detection."""
        callback = Mock()
        monitor.register_observer(callback)

        monitor._process_line("Error: Something failed")

        # Should receive both output_line and error events
        callback.assert_called()
        assert callback.call_count >= 1

    def test_multiple_observers_all_notified(self, monitor):
        """Test all observers are notified."""
        callback1 = Mock()
        callback2 = Mock()
        callback3 = Mock()

        monitor.register_observer(callback1)
        monitor.register_observer(callback2)
        monitor.register_observer(callback3)

        monitor._process_line("New output")

        callback1.assert_called()
        callback2.assert_called()
        callback3.assert_called()

    def test_observer_exception_handling(self, monitor):
        """Test that observer exceptions don't break monitoring."""
        failing_callback = Mock(side_effect=Exception("Observer error"))
        working_callback = Mock()

        monitor.register_observer(failing_callback)
        monitor.register_observer(working_callback)

        # Should not raise exception
        monitor._process_line("Test output")

        # Working callback should still be called
        working_callback.assert_called()


class TestResponseExtraction:
    """Test get_response functionality."""

    @pytest.fixture
    def monitor(self):
        """Create monitor instance for testing."""
        mon = OutputMonitor()
        yield mon
        # Safety cleanup: stop monitoring if still active
        if mon.is_monitoring:
            mon.stop_monitoring()

    def test_get_response_simple(self, monitor):
        """Test getting simple response."""
        lines = ["Line 1", "Line 2", "Line 3"]

        for line in lines:
            monitor._process_line(line)

        response = monitor.get_response()

        assert "Line 1" in response
        assert "Line 2" in response
        assert "Line 3" in response

    def test_get_response_empty_buffer(self, monitor):
        """Test getting response from empty buffer."""
        response = monitor.get_response()

        assert response == ""

    def test_get_response_preserves_order(self, monitor):
        """Test that response preserves line order."""
        lines = ["First", "Second", "Third"]

        for line in lines:
            monitor._process_line(line)

        response = monitor.get_response()

        # Check order is preserved
        first_pos = response.find("First")
        second_pos = response.find("Second")
        third_pos = response.find("Third")

        assert first_pos < second_pos < third_pos

    def test_get_response_includes_all_lines(self, monitor):
        """Test that response includes all buffer lines."""
        for i in range(50):
            monitor._process_line(f"Line {i}")

        response = monitor.get_response()

        # All lines should be present
        for i in range(50):
            assert f"Line {i}" in response


class TestStartStopMonitoring:
    """Test starting and stopping monitoring."""

    @pytest.fixture
    def monitor(self):
        """Create monitor instance for testing."""
        mon = OutputMonitor()
        yield mon
        # Safety cleanup: stop monitoring if still active
        if mon.is_monitoring:
            mon.stop_monitoring()

    def test_start_monitoring_invalid_stream(self, monitor):
        """Test starting monitoring with invalid stream."""
        invalid_stream = "not a stream"

        with pytest.raises(MonitoringException) as exc_info:
            monitor.start_monitoring(invalid_stream)

        assert 'Invalid output stream' in str(exc_info.value)

    def test_start_monitoring_already_active(self, monitor):
        """Test starting monitoring when already active."""
        mock_stream = Mock()
        mock_stream.readline = Mock(return_value="")
        mock_stream.recv_ready = Mock(return_value=False)

        monitor.start_monitoring(mock_stream)

        with pytest.raises(MonitoringException) as exc_info:
            monitor.start_monitoring(mock_stream)

        assert 'already active' in str(exc_info.value).lower()

        monitor.stop_monitoring()

    def test_stop_monitoring(self, monitor):
        """Test stopping monitoring."""
        mock_stream = Mock()
        mock_stream.readline = Mock(return_value="")
        mock_stream.recv_ready = Mock(return_value=False)

        monitor.start_monitoring(mock_stream)
        time.sleep(0.1)

        monitor.stop_monitoring()

        assert not monitor.is_monitoring

    def test_stop_monitoring_when_not_active(self, monitor):
        """Test stopping monitoring when not active."""
        # Should not raise exception
        monitor.stop_monitoring()

        assert not monitor.is_monitoring

    def test_context_manager(self):
        """Test using monitor as context manager."""
        mock_stream = Mock()
        mock_stream.readline = Mock(return_value="")
        mock_stream.recv_ready = Mock(return_value=False)

        with OutputMonitor() as monitor:
            monitor.start_monitoring(mock_stream)
            assert monitor.is_monitoring

        # Should auto-stop after context
        assert not monitor.is_monitoring


class TestEdgeCases:
    """Test edge cases and error conditions."""

    def test_empty_line_handling(self):
        """Test handling of empty lines."""
        monitor = OutputMonitor()

        monitor._process_line("")

        assert len(monitor._buffer) >= 0

    def test_very_long_line_handling(self):
        """Test handling of very long lines."""
        monitor = OutputMonitor()

        # Reduced from 100KB to 15KB to comply with TEST_GUIDELINES.md (max 20KB)
        long_line = "x" * 15000
        monitor._process_line(long_line)

        buffer = monitor.get_buffer(lines=100)
        assert len(buffer) == 1
        assert len(buffer[0]) == 15000

    def test_unicode_handling(self):
        """Test handling of unicode characters."""
        monitor = OutputMonitor()

        unicode_text = "Hello ä¸–ç•Œ ðŸŒ"
        monitor._process_line(unicode_text)

        response = monitor.get_response()
        assert "ä¸–ç•Œ" in response
        assert "ðŸŒ" in response

    def test_special_characters_in_patterns(self):
        """Test patterns with special regex characters."""
        monitor = OutputMonitor()

        # Add output with special characters
        output = "Processing [DONE] with 100% success (finally)!"
        monitor._process_line(output)

        # Should not cause regex errors
        response = monitor.get_response()
        assert response is not None

    def test_concurrent_is_complete_calls(self):
        """Test concurrent calls to is_complete."""
        monitor = OutputMonitor(completion_timeout=0.2)
        monitor._process_line("Ready for next")

        results = []
        errors = []

        def check_complete():
            try:
                time.sleep(0.25)  # Wait slightly longer than timeout
                result = monitor.is_complete()
                results.append(result)
            except Exception as e:
                errors.append(e)

        # Reduced from 5 to 3 threads
        threads = [threading.Thread(target=check_complete) for _ in range(3)]
        for thread in threads:
            thread.start()
        for thread in threads:
            thread.join(timeout=2.0)  # Add timeout

        assert len(errors) == 0
        assert all(results)  # All should see completion

    def test_none_line_handling(self):
        """Test handling of None lines."""
        monitor = OutputMonitor()

        # The _read_line_with_timeout returns None, but _process_line shouldn't be called
        # Test that get_response works even if no lines added
        response = monitor.get_response()
        assert response == ""


class TestRealWorldScenarios:
    """Test realistic usage scenarios."""

    def test_typical_command_execution(self, fast_time):
        """Test monitoring a typical command execution."""
        monitor = OutputMonitor()

        # Simulate command output
        output_sequence = [
            "Starting process...",
            "Loading modules...",
            "Processing data...",
            "Writing output...",
            "Command completed",
            "Ready for next"
        ]

        for line in output_sequence:
            monitor._process_line(line)
            time.sleep(0.1)

        # Wait for completion timeout (2.0s default)
        time.sleep(2.1)

        assert monitor.is_complete()
        response = monitor.get_response()
        assert "Starting process" in response
        assert "Command completed" in response

    def test_command_with_error(self):
        """Test monitoring command that produces error."""
        monitor = OutputMonitor()

        output_sequence = [
            "Starting process...",
            "Error: File not found: config.yaml",
            "Traceback (most recent call last):",
            "  File 'main.py', line 42",
            "FileNotFoundError: config.yaml",
            "Command FAILED"
        ]

        for line in output_sequence:
            monitor._process_line(line)

        error = monitor.detect_error()
        assert error is not None
        assert "Error" in error or "FAILED" in error or "File not found" in error

    def test_command_with_rate_limit(self):
        """Test monitoring command that hits rate limit."""
        monitor = OutputMonitor()

        output_sequence = [
            "Making API request...",
            "Error 429: rate limit exceeded",
            "Please try again in 60 seconds"
        ]

        for line in output_sequence:
            monitor._process_line(line)

        assert monitor.detect_rate_limit()
        error = monitor.detect_error()
        assert error is not None

    def test_long_running_process(self, fast_time):
        """Test monitoring long-running process with progress updates."""
        monitor = OutputMonitor()

        # Simulate progress updates
        for i in range(0, 101, 10):
            monitor._process_line(f"Progress: {i}%")
            time.sleep(0.05)

        monitor._process_line("Finished successfully")
        # Wait for completion timeout (2.0s default)
        time.sleep(2.1)

        assert monitor.is_complete()
        response = monitor.get_response()
        assert "100%" in response
        assert "Finished" in response

    def test_multiple_commands_in_sequence(self, fast_time):
        """Test monitoring multiple commands executed sequentially."""
        monitor = OutputMonitor()

        # First command
        monitor._process_line("Command 1 starting")
        monitor._process_line("Command 1 done")
        monitor._process_line("Ready for next")

        # Wait for completion timeout (2.0s default)
        time.sleep(2.1)
        assert monitor.is_complete()

        # Clear for next command
        monitor.clear_buffer()
        monitor._completion_event.clear()
        monitor._completion_marker_found = False
        # Also need to reset last output time
        monitor._last_output_time = 0.0

        # Second command
        monitor._process_line("Command 2 starting")
        monitor._process_line("Command 2 done")
        monitor._process_line(">>>")

        time.sleep(0.3)
        assert monitor.is_complete()


class TestPerformance:
    """Test performance characteristics."""

    def test_large_buffer_performance(self):
        """Test performance with large buffer."""
        monitor = OutputMonitor(buffer_size=5000)

        start_time = time.time()

        # Add 1000 lines (reduced from 10000)
        for i in range(1000):
            monitor._process_line(f"Line {i}")

        elapsed = time.time() - start_time

        # Should complete in reasonable time (< 1 second)
        assert elapsed < 1.0

    def test_pattern_matching_performance(self):
        """Test pattern matching performance."""
        monitor = OutputMonitor()

        # Add lines without patterns (reduced from 1000 to 500)
        for i in range(500):
            monitor._process_line(f"Normal output line {i}")

        start_time = time.time()

        # Check for errors/rate limits
        monitor.detect_error()
        monitor.detect_rate_limit()

        elapsed = time.time() - start_time

        # Pattern matching should be fast (< 100ms)
        assert elapsed < 0.1

    def test_observer_notification_overhead(self):
        """Test overhead of observer notifications."""
        monitor = OutputMonitor()

        # Add 5 observers (reduced from 10)
        for _ in range(5):
            monitor.register_observer(Mock())

        start_time = time.time()

        # Add 50 lines (reduced from 100)
        for i in range(50):
            monitor._process_line(f"Line {i}")

        elapsed = time.time() - start_time

        # Should still be reasonably fast (< 1 second)
        assert elapsed < 1.0


class TestAcceptanceCriteria:
    """Test acceptance criteria from specification."""

    def test_completion_detection_accuracy(self):
        """Test >95% accuracy of completion detection."""
        monitor = OutputMonitor(completion_timeout=0.2)

        # Test various completion scenarios
        test_cases = [
            ("Ready for next command", True),
            (">>> waiting for input", True),  # >>> marker with context
            ("Command completed successfully", True),
            ("Finished processing", True),
            ("Done", True),  # Contains "Done" marker
            ("All tasks Done!", True),  # Another Done case
            ("Still working on the problem...", False),
            ("Processing data now...", False),
            ("Starting new task", False),
            ("Analyzing results", False),
            ("Abandoned the task", False),  # Contains "Done" as substring but not marker
        ]

        correct_detections = 0

        for output, should_complete in test_cases:
            # Create new monitor for each test
            test_monitor = OutputMonitor(completion_timeout=0.2)
            test_monitor._process_line(output.strip())

            if should_complete:
                time.sleep(0.25)  # Slightly longer than timeout

            is_complete = test_monitor.is_complete()

            if is_complete == should_complete:
                correct_detections += 1

        accuracy = correct_detections / len(test_cases)
        assert accuracy >= 0.95, f"Accuracy {accuracy} < 0.95"

    def test_error_detection_catches_all_types(self):
        """Test error detection catches all error types."""
        monitor = OutputMonitor()

        error_outputs = [
            ("Error: Something failed", "Error"),
            ("Exception: ValueError occurred", "Exception"),
            ("Traceback: error in function", "Traceback"),
            ("Test suite FAILED completely", "FAILED"),
            ("AssertionError: assertion failed", "AssertionError")
        ]

        for error_output, expected_marker in error_outputs:
            test_monitor = OutputMonitor()
            test_monitor._process_line(error_output)

            error = test_monitor.detect_error()
            assert error is not None, f"Failed to detect: {error_output}"
            assert expected_marker.lower() in error.lower(), f"Expected {expected_marker} in {error}"

    def test_rate_limit_detection_reliable(self):
        """Test rate limit detection is reliable."""
        monitor = OutputMonitor()

        rate_limit_outputs = [
            "rate limit exceeded",
            "too many requests",
            "try again later",
            "quota exceeded"
        ]

        for output in rate_limit_outputs:
            test_monitor = OutputMonitor()
            test_monitor._process_line(output)

            is_rate_limited = test_monitor.detect_rate_limit()
            assert is_rate_limited, f"Failed to detect: {output}"

    def test_minimal_false_positives(self):
        """Test <5% false positive rate."""
        monitor = OutputMonitor()

        # Normal outputs that should not trigger detection
        # Use outputs that don't contain trigger words at all
        normal_outputs = [
            "Processing file main.py",  # No trigger words
            "Starting the process now",  # Contains "start" but not "ready for"
            "Setting output limit to 100",  # Contains "limit" but context is different
            "Exceptional performance achieved",  # Contains "exception" as substring only
            "Running command analyzer",  # Not a completion marker
            "Building artifacts directory",  # No trigger words
            "Loading configuration",  # No trigger words
            "Analyzing dependencies",  # No trigger words
            "Compiling source files",  # No trigger words
            "Deploying to staging",  # No trigger words
        ]

        false_positives = 0

        for output in normal_outputs:
            test_monitor = OutputMonitor(completion_timeout=0.2)
            test_monitor._process_line(output)

            # Should not detect completion, error, or rate limit
            time.sleep(0.25)  # Slightly longer than timeout
            if (test_monitor.is_complete() or
                test_monitor.detect_error() is not None or
                test_monitor.detect_rate_limit()):
                false_positives += 1
                print(f"False positive for: {output}")

        false_positive_rate = false_positives / len(normal_outputs)
        assert false_positive_rate < 0.05, f"FP rate {false_positive_rate} >= 0.05"

    def test_buffer_prevents_overflow(self):
        """Test buffer management prevents overflow."""
        monitor = OutputMonitor(buffer_size=500)

        # Add way more lines than buffer size (reduced from 5000 to 1500)
        for i in range(1500):
            monitor._process_line(f"Line {i}")

        # Buffer should be at max size
        assert len(monitor._buffer) == 500

        # Should still be functional
        response = monitor.get_response()
        assert response is not None
        assert len(response) > 0


class TestMonitoringStatistics:
    """Test monitoring statistics and helper methods."""

    def test_get_stats(self):
        """Test getting monitoring statistics."""
        monitor = OutputMonitor()

        monitor._process_line("Test line")

        stats = monitor.get_stats()

        assert 'is_monitoring' in stats
        assert 'buffer_lines' in stats
        assert 'completion_detected' in stats
        assert 'error_detected' in stats
        assert 'rate_limit_detected' in stats
        assert 'idle_seconds' in stats
        assert 'observers_count' in stats

    def test_wait_for_completion(self):
        """Test waiting for completion with timeout."""
        monitor = OutputMonitor(completion_timeout=0.2)

        # Start a thread that will mark completion
        def mark_complete():
            time.sleep(0.05)
            monitor._process_line("Ready for next")
            # Wait for completion timeout
            time.sleep(0.25)
            # Trigger completion check
            monitor.is_complete()

        thread = threading.Thread(target=mark_complete)
        thread.start()

        # Wait for completion (should succeed)
        result = monitor.wait_for_completion(timeout=1.0)

        thread.join(timeout=2.0)  # Add timeout

        assert result is True

    def test_wait_for_completion_timeout(self):
        """Test wait_for_completion with timeout exceeded."""
        monitor = OutputMonitor()

        # Don't add completion marker
        result = monitor.wait_for_completion(timeout=0.5)

        assert result is False


if __name__ == '__main__':
    pytest.main([__file__, '-v'])
