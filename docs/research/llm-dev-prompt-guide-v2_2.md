# LLM Development Agent Prompt Engineering Guide v2.2

**A comprehensive framework for high-quality, consistent AI-assisted software development**

**Security & Cost-Optimized Edition**: Enhanced with prompt injection defenses, accurate cost modeling, and production-hardened validation

---

## Quick Start

**New to this guide?** Start here:
1. Read [Core Principles](#core-principles) (5 min)
2. Copy [Planning Prompt Template](#planning-prompt-template-pasteable) (2 min)  
3. Review [Quick Reference Card](#quick-reference-card) (2 min)
4. Review [Before/After Example](#beforeafter-example) (3 min)
5. Customize for your tech stack and run your first planning session

**Experienced user?** Jump to:
- [Prompt Injection & Tool-Use Safety](#prompt-injection--tool-use-safety) for security controls ðŸ†•
- [Cost Modeling & Budgeting](#cost-modeling--budgeting) for accurate budget planning ðŸ”„
- [Decision Trees](#decision-trees) for daily decision support
- [Enforcement Validation](#complete-enforcement-validation) for CI/CD integration ðŸ”„
- [Advanced Techniques](#advanced-techniques) for RAG, multi-agent, and optimization
- [Quick Reference Card](#quick-reference-card) for one-page cheat sheet

---

## Executive Summary

This guide establishes a robust prompt architecture for instructing LLM agents in software development tasks, ensuring consistent, high-quality outputs through structured prompts, verification gates, comprehensive state management, and modern AI engineering practices.

**What's New in v2.2 (Security & Cost-Optimized Edition):**
- ðŸ†• **Prompt Injection & Tool-Use Safety** - Comprehensive controls for RAG and function calling
- ðŸ”„ **Fixed Schema/Validator Alignment** - All examples now match validation schemas
- ðŸ”„ **Accurate Cost Modeling** - Separate functions for first-call and cached-call costs
- ðŸ”„ **Reasoning Privacy** - Decision records instead of raw chain-of-thought in outputs
- ðŸ”„ **Enhanced CI Portability** - Works across Node.js, Python, and other ecosystems
- ðŸ”„ **Determinism & Sampling Guidance** - Standards for reproducible outputs
- ðŸ”„ **Parameterized Models & Pricing** - No more hard-coded values
- âœ… Removed duplicate sections and stubs

**Core Features:**
- Decision record patterns (ADR) for auditable reasoning
- Multi-agent orchestration strategies  
- Secure RAG and tool-use integration
- Prompt versioning and regression testing
- Enhanced observability with privacy controls
- Modern structured output patterns
- Comprehensive error recovery strategies
- Production operations guidance

---

## Table of Contents

1. [Core Principles](#core-principles)
2. [Prompt Architecture](#prompt-architecture)
3. [Planning & Execution Framework](#planning--execution-framework)
4. [Context & Token Management](#context--token-management)
5. [Testing & Verification](#testing--verification)
6. [Multi-Agent Patterns](#multi-agent-patterns)
7. [Advanced Techniques](#advanced-techniques)
8. [Prompt Injection & Tool-Use Safety](#prompt-injection--tool-use-safety) ðŸ†•
9. [Security & Compliance](#security--compliance)
10. [Observability & Metrics](#observability--metrics) ðŸ”„
11. [Error Handling & Recovery](#error-handling--recovery)
12. [Templates & Examples](#templates--examples)
13. [Specialized Task Patterns](#specialized-task-patterns)
14. [Cost Modeling & Budgeting](#cost-modeling--budgeting) ðŸ”„
15. [Production Operations](#production-operations)
16. [Checklist & Governance](#checklist--governance) ðŸ”„
17. [Appendix](#appendix)

---

## Core Principles

### Foundational Rules

1. **Explicit Role and Scope** â€” Define clear boundaries for agent authority and responsibilities
2. **Separate Planning from Execution** â€” Never generate code without an approved plan
3. **Dual Output Format** â€” Always produce human-readable AND machine-readable artifacts
4. **Verification Gates** â€” Enforce checkpoints before proceeding to next phase
5. **Idempotent, Data-Driven Design** â€” No hardcoded values; prefer configuration and schemas
6. **Context Window Management** â€” Plan for state chunking and rehydration across windows
7. **Decision Record Reasoning** â€” Require explicit, auditable decision records (not raw chain-of-thought)
8. **Auditability First** â€” Log all decisions, changes, and rationales with timestamps
9. **Security by Default** â€” Declare and enforce constraints for data, dependencies, and network access
10. **Test as First-Class Work** â€” Documentation and tests are mandatory deliverables, not afterthoughts

### Reasoning Patterns (v2.2 Updated)

**CRITICAL PRIVACY RULE**: Never log or persist raw internal chain-of-thought reasoning. Store only concise decision rationales, assumptions, and alternatives in structured Decision Records.

**Decision Records (ADR Pattern)**: For complex tasks, create auditable decision records:
```markdown
## Decision Record: [Feature/Component Name]

**Date**: [ISO-8601]
**Status**: [Proposed|Accepted|Deprecated|Superseded]

### Context
[Brief description of the situation and problem]

### Decision
[The decision made]

### Consequences
**Positive**: [Benefits and advantages]
**Negative**: [Drawbacks and risks]
**Mitigations**: [How negative consequences are addressed]

### Alternatives Considered
1. **[Option A]**: [Brief evaluation - rejected because...]
2. **[Option B]**: [Brief evaluation - rejected because...]

### Assumptions
- [Key assumption 1]
- [Key assumption 2]
```

**Internal Reasoning (Private Scratchpad)**: Use for working memory ONLY - never output:
- Agent may use internal reasoning for problem-solving
- Must be stripped before any output or logging
- Think of it as "scratch paper" - useful during work, not part of deliverable

**Structured Alternatives Evaluation**: For multi-path decisions:
```markdown
## Options Analysis: [Decision Point]

| Criteria | Option A | Option B | Option C | Weight |
|----------|----------|----------|----------|--------|
| Performance | Good (8/10) | Excellent (9/10) | Fair (6/10) | High |
| Maintainability | Excellent (9/10) | Good (7/10) | Good (8/10) | High |
| Cost | Low ($) | High ($$$) | Medium ($$) | Medium |
| Implementation Time | 2 weeks | 4 weeks | 1 week | High |

**Recommendation**: Option A - Best balance of performance, maintainability, and implementation speed.
```

**Reflection and Self-Correction**: Build in verification loops:
```
After generating code:
1. Review against acceptance criteria â†’ [Pass/Fail with specific gaps]
2. Identify potential issues â†’ [List specific concerns]
3. Self-correct if needed â†’ [Log what was changed and why in decision record]
4. Document corrections â†’ [Add to decision record, not raw reasoning]
```

---

## Prompt Architecture

### Canonical Prompt Structure

Every prompt should contain these sections in order:

```markdown
## 1. AGENT IDENTITY & PERMISSIONS

You are [role] with authority to [capabilities].
You CANNOT [restrictions] without human approval.
Version: [prompt-version] | Updated: [date]

## 2. OBJECTIVE & DESIGN INTENT

**Primary Goal**: [One clear sentence]

**User Stories**:
- As a [user], I want [capability], so that [benefit]
- [2-5 total stories]

**Success Definition**: [Observable outcome that defines done]

## 3. CONTEXT & CONSTRAINTS

**Technical Stack**:
- Languages/Frameworks: [with versions]
- Infrastructure: [cloud, containers, etc.]
- Repository: [url], Branch Policy: [pattern]
- CI/CD: [tools and pipeline stages]

**Style & Standards**:
- Linter: [tool + config file]
- Formatter: [tool + rules]
- Code Style Guide: [link or inline key rules]
- Test Coverage: [minimum % or criteria]

**Performance Targets**:
- Latency: [p95 <= X ms]
- Throughput: [requests/sec]
- Resource Limits: [memory, CPU]

**Security & Compliance**:
- Data Classification: [PII, sensitive, public]
- Required Encryption: [at-rest, in-transit]
- Auth/Authz: [mechanism and scopes]
- Regulatory: [GDPR, HIPAA, SOC2, etc.]
- Forbidden: [libraries, patterns, practices]

**Dependency Policy**:
- Approved: [list or registry]
- Require Pre-Approval: [any new deps]
- Vulnerability Scanning: [tool and threshold]

## 4. REASONING REQUIREMENTS

Before generating any code or design:
1. State your understanding of the problem
2. List key assumptions and confirm with human if uncertain
3. Create a Decision Record (ADR) evaluating 2-3 approaches with tradeoffs
4. Explain your recommended approach and why in the decision record
5. Identify risks and mitigation strategies in the decision record
6. NEVER output raw internal reasoning or chain-of-thought

## 5. DELIVERABLES & FORMATS

**Human-Readable** (Markdown):
- Design Document: docs/[feature]/design.md
- Implementation Plan: docs/[feature]/plan.md
- Decision Records: docs/decisions/ADR-NNN-[title].md
- Runbook: docs/[feature]/runbook.md

**Machine-Readable** (JSON/YAML):
- Plan Manifest: plan_manifest.json [must follow schema v2.2]
- Acceptance Criteria: acceptance.yaml
- Test Spec: test_spec.json

**Code Artifacts**:
- Source: src/[paths]
- Tests: tests/unit/, tests/integration/, tests/e2e/
- Config: config/[environment].yaml
- Migrations: migrations/[timestamp]_[description].sql
- Infrastructure: infra/[terraform|cloudformation]/

**Changelog & Commits**:
- CHANGELOG.md: entry per semantic versioning
- Commit Message Template: "[Epic-ID] [type]: [short] â€” [detail]"
- Types: feat, fix, refactor, test, docs, chore, perf, security

## 6. ACCEPTANCE CRITERIA

**Definition of Done**: All must pass:
- [ ] Unit tests: [coverage >= X%]
- [ ] Integration tests: [all pass]
- [ ] Linter: [zero errors, warnings optional]
- [ ] Security scan: [no critical/high vulnerabilities]
- [ ] Performance: [meets targets in section 3]
- [ ] Documentation: [complete and reviewed]
- [ ] Accessibility: [WCAG 2.1 AA if UI] (if applicable)
- [ ] Manual verification: [smoke test steps provided and executed]

**Feature-Specific Criteria**:
- [Custom acceptance criteria for this feature]

## 7. PLANNING RULES

**Framework**: Organize as Epic â†’ Stories â†’ Tasks â†’ Subtasks

**Dependency Management**:
- Sequence: critical path first, parallel where possible
- Risk weighting: high-risk/high-value tasks early
- Stop points: size phases for [N tokens] or [M files] per window

**Plan Review Gates**:
1. Self-review: agent validates plan against requirements
2. Machine validation: schema compliance, dependency checks
3. Human approval: explicit APPROVED_BY with signature and date
4. Version control: commit plan with [Epic-ID] prefix

## 8. EXECUTION GATES

**Phase Transition Checklist** (Must verify before proceeding):
- [ ] Current phase complete (all tasks done)
- [ ] All artifacts generated and validated
- [ ] Context window below [threshold]%
- [ ] No blockers or unresolved dependencies
- [ ] Human approval obtained if required
- [ ] State manifest updated and committed

## 9. ERROR HANDLING & RECOVERY

**Failure Response Protocol**:
1. Capture full error context (message, stack, state)
2. Log to structured format (JSON with timestamp, severity, task_id)
3. Create decision record documenting the error and resolution
4. Attempt auto-recovery (if retryable and < max_attempts)
5. If unrecoverable: HALT + report to human with context
6. Update plan_manifest.json with error log reference

**Rollback Procedure**:
- Always provide rollback steps in commit message
- Keep previous version artifacts until new version validated
- Document rollback in decision record

## 10. OBSERVABILITY HOOKS

**Required Instrumentation**:
- Structured logs: JSON format with correlation IDs
- Metrics: task duration, token usage, error rates
- Traces: span each major operation (plan, code gen, validation)
- Audits: decision records stored in docs/decisions/

**Privacy & Redaction**:
- NEVER log raw internal reasoning or chain-of-thought
- Redact PII, secrets, tokens before logging
- Log only decision summaries and outcomes
- Retention: Decision records (indefinite), Debug logs (30 days), Trace data (7 days)

## 11. SECURITY CONTROLS

**Input Validation**:
- Validate all prompts against injection patterns (see Prompt Injection section)
- Sanitize user-provided content before processing
- Use allowlists for tool/function names

**Tool Execution Safety**:
- Require schema validation for all tool inputs
- Implement confirmation workflow for destructive operations
- Tag RAG documents with origin/classification
- Apply least-privilege principle to tool permissions

**Output Sanitization**:
- Strip any accidentally included sensitive data
- Validate structured outputs match expected schema
- Never echo raw user input in responses
```

---

## Planning & Execution Framework

### Workflow Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 0: Requirements Gathering                              â”‚
â”‚ â€¢ Load context from user/docs/previous sessions              â”‚
â”‚ â€¢ Ask clarifying questions (2-3 max, focused)                â”‚
â”‚ â€¢ Confirm understanding before proceeding                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 1: Planning (MANDATORY - CANNOT BE SKIPPED)           â”‚
â”‚ â€¢ Create Decision Record (ADR) for approach                  â”‚
â”‚ â€¢ Generate plan_manifest.json (machine-readable)             â”‚
â”‚ â€¢ Generate implementation plan (human-readable)              â”‚
â”‚ â€¢ WAIT for explicit approval: APPROVED_BY: <name> <date>    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                   [Approval Gate]
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 2: Execution (ONLY AFTER APPROVAL)                    â”‚
â”‚ â€¢ Load approved plan_manifest.json                           â”‚
â”‚ â€¢ Generate code following plan                               â”‚
â”‚ â€¢ Write tests (unit, integration, e2e as needed)             â”‚
â”‚ â€¢ Run validation pipeline (lint, test, security)             â”‚
â”‚ â€¢ Create decision records for implementation choices         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 3: Verification                                        â”‚
â”‚ â€¢ All quality gates pass                                     â”‚
â”‚ â€¢ Documentation complete                                     â”‚
â”‚ â€¢ Manual smoke test steps provided                           â”‚
â”‚ â€¢ Create PR with rollback instructions                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 4: Reflection & State Update                          â”‚
â”‚ â€¢ Update plan_manifest.json (mark tasks complete)            â”‚
â”‚ â€¢ Record any deviations in decision records                  â”‚
â”‚ â€¢ Generate phase summary (â‰¤500 tokens)                       â”‚
â”‚ â€¢ If more work: loop to Phase 1 for next epic/story         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Plan Manifest Schema v2.2 (CORRECTED)

**CRITICAL**: This schema has been updated to match validator expectations.

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "LLM Agent Plan Manifest",
  "version": "2.2",
  "type": "object",
  "required": ["meta", "objective", "constraints", "dependencies", "plan", "state"],
  "properties": {
    "meta": {
      "type": "object",
      "required": ["epic_id", "created_at", "prompt_version", "agent_version"],
      "properties": {
        "epic_id": {"type": "string", "pattern": "^[A-Z]+-[0-9]+$"},
        "created_at": {"type": "string", "format": "date-time"},
        "updated_at": {"type": "string", "format": "date-time"},
        "prompt_version": {"type": "string", "pattern": "^[0-9]+\\.[0-9]+$"},
        "agent_version": {"type": "string"},
        "approved_by": {"type": "string"},
        "approved_at": {"type": "string", "format": "date-time"}
      }
    },
    "objective": {
      "type": "object",
      "required": ["summary", "user_stories", "success_criteria"],
      "properties": {
        "summary": {"type": "string", "maxLength": 200},
        "user_stories": {
          "type": "array",
          "minItems": 1,
          "items": {"type": "string"}
        },
        "success_criteria": {
          "type": "array",
          "minItems": 1,
          "items": {"type": "string"}
        }
      }
    },
    "constraints": {
      "type": "object",
      "properties": {
        "max_tokens_per_phase": {"type": "integer", "minimum": 1000},
        "max_files_per_commit": {"type": "integer", "minimum": 1},
        "context_threshold_pct": {"type": "integer", "minimum": 50, "maximum": 90},
        "test_coverage_min_pct": {"type": "integer", "minimum": 0, "maximum": 100},
        "performance_targets": {
          "type": "object",
          "properties": {
            "p95_latency_ms": {"type": "integer"},
            "throughput_rps": {"type": "integer"},
            "max_memory_mb": {"type": "integer"}
          }
        },
        "security": {
          "type": "object",
          "properties": {
            "data_classification": {"type": "string", "enum": ["public", "internal", "confidential", "restricted"]},
            "encryption_required": {"type": "array", "items": {"type": "string", "enum": ["at-rest", "in-transit"]}},
            "regulatory_requirements": {"type": "array", "items": {"type": "string"}}
          }
        }
      }
    },
    "dependencies": {
      "type": "object",
      "properties": {
        "existing_packages": {
          "type": "array",
          "items": {
            "type": "object",
            "required": ["name", "version"],
            "properties": {
              "name": {"type": "string"},
              "version": {"type": "string"},
              "ecosystem": {"type": "string"}
            }
          }
        },
        "new_packages": {
          "type": "array",
          "items": {
            "type": "object",
            "required": ["name", "version", "justification"],
            "properties": {
              "name": {"type": "string"},
              "version": {"type": "string"},
              "ecosystem": {"type": "string"},
              "justification": {"type": "string"},
              "alternatives_considered": {
                "type": "array",
                "items": {"type": "string"}
              },
              "security_scan_status": {"type": "string", "enum": ["pending", "passed", "failed"]},
              "license": {"type": "string"}
            }
          }
        },
        "tools_and_resources": {
          "type": "array",
          "items": {
            "type": "object",
            "required": ["name", "purpose"],
            "properties": {
              "name": {"type": "string"},
              "purpose": {"type": "string"},
              "permissions_required": {"type": "array", "items": {"type": "string"}},
              "configuration": {"type": "object"}
            }
          }
        }
      }
    },
    "plan": {
      "type": "object",
      "required": ["phases"],
      "properties": {
        "phases": {
          "type": "array",
          "minItems": 1,
          "items": {
            "type": "object",
            "required": ["phase_id", "name", "stories"],
            "properties": {
              "phase_id": {"type": "string"},
              "name": {"type": "string"},
              "description": {"type": "string"},
              "estimated_tokens": {"type": "integer"},
              "estimated_duration_hours": {"type": "number"},
              "stories": {
                "type": "array",
                "items": {
                  "type": "object",
                  "required": ["story_id", "description", "tasks"],
                  "properties": {
                    "story_id": {"type": "string"},
                    "description": {"type": "string"},
                    "acceptance_criteria": {"type": "array", "items": {"type": "string"}},
                    "tasks": {
                      "type": "array",
                      "items": {
                        "type": "object",
                        "required": ["task_id", "description", "type"],
                        "properties": {
                          "task_id": {"type": "string"},
                          "description": {"type": "string"},
                          "type": {"type": "string", "enum": ["code", "test", "doc", "config", "review"]},
                          "estimated_tokens": {"type": "integer"},
                          "dependencies": {"type": "array", "items": {"type": "string"}},
                          "artifacts": {"type": "array", "items": {"type": "string"}},
                          "validation_rules": {"type": "array", "items": {"type": "string"}}
                        }
                      }
                    }
                  }
                }
              }
            }
          }
        },
        "decision_records": {
          "type": "array",
          "items": {
            "type": "object",
            "required": ["id", "title", "decision", "status"],
            "properties": {
              "id": {"type": "string"},
              "title": {"type": "string"},
              "decision": {"type": "string"},
              "rationale": {"type": "string"},
              "alternatives": {"type": "array", "items": {"type": "string"}},
              "consequences": {"type": "object"},
              "status": {"type": "string", "enum": ["proposed", "accepted", "deprecated", "superseded"]}
            }
          }
        }
      }
    },
    "state": {
      "type": "object",
      "required": ["current_phase", "current_story", "completed_tasks", "status"],
      "properties": {
        "current_phase": {"type": "string"},
        "current_story": {"type": "string"},
        "completed_tasks": {"type": "array", "items": {"type": "string"}},
        "blocked_tasks": {
          "type": "array",
          "items": {
            "type": "object",
            "required": ["task_id", "reason"],
            "properties": {
              "task_id": {"type": "string"},
              "reason": {"type": "string"},
              "blocked_since": {"type": "string", "format": "date-time"}
            }
          }
        },
        "status": {"type": "string", "enum": ["planning", "approved", "in_progress", "blocked", "completed", "failed"]},
        "context_usage": {
          "type": "object",
          "properties": {
            "tokens_used": {"type": "integer"},
            "tokens_available": {"type": "integer"},
            "percentage": {"type": "number", "minimum": 0, "maximum": 100}
          }
        },
        "validation_results": {
          "type": "object",
          "properties": {
            "lint": {"type": "object"},
            "tests": {"type": "object"},
            "security": {"type": "object"},
            "performance": {"type": "object"}
          }
        }
      }
    }
  }
}
```

### Example Plan Manifest (v2.2 Compliant)

```json
{
  "meta": {
    "epic_id": "AUTH-001",
    "created_at": "2025-11-11T10:00:00Z",
    "updated_at": "2025-11-11T14:30:00Z",
    "prompt_version": "2.2",
    "agent_version": "claude-sonnet-4-20250514",
    "approved_by": "Jane Doe",
    "approved_at": "2025-11-11T11:00:00Z"
  },
  "objective": {
    "summary": "Implement JWT-based user authentication with refresh token support",
    "user_stories": [
      "As a user, I want to log in with email and password",
      "As a user, I want my session to persist securely",
      "As a user, I want to log out and invalidate my session"
    ],
    "success_criteria": [
      "Authentication endpoint accepts credentials and returns JWT",
      "Token refresh mechanism works correctly",
      "Logout invalidates refresh tokens",
      "All security tests pass with zero high/critical vulnerabilities"
    ]
  },
  "constraints": {
    "max_tokens_per_phase": 50000,
    "max_files_per_commit": 10,
    "context_threshold_pct": 70,
    "test_coverage_min_pct": 80,
    "performance_targets": {
      "p95_latency_ms": 100,
      "throughput_rps": 1000,
      "max_memory_mb": 512
    },
    "security": {
      "data_classification": "confidential",
      "encryption_required": ["at-rest", "in-transit"],
      "regulatory_requirements": ["GDPR", "SOC2"]
    }
  },
  "dependencies": {
    "existing_packages": [
      {
        "name": "express",
        "version": "4.18.2",
        "ecosystem": "npm"
      }
    ],
    "new_packages": [
      {
        "name": "jsonwebtoken",
        "version": "9.0.2",
        "ecosystem": "npm",
        "justification": "Industry standard for JWT implementation with active maintenance",
        "alternatives_considered": [
          "jose (newer but smaller ecosystem)",
          "passport-jwt (heavier, includes unnecessary features)"
        ],
        "security_scan_status": "passed",
        "license": "MIT"
      },
      {
        "name": "bcrypt",
        "version": "5.1.1",
        "ecosystem": "npm",
        "justification": "Secure password hashing with proven track record",
        "alternatives_considered": [
          "argon2 (better but harder to deploy)",
          "scrypt (native but less battle-tested)"
        ],
        "security_scan_status": "passed",
        "license": "MIT"
      }
    ],
    "tools_and_resources": [
      {
        "name": "Redis",
        "purpose": "Store refresh token blacklist for logout",
        "permissions_required": ["read", "write", "delete"],
        "configuration": {
          "host": "redis.internal",
          "port": 6379,
          "db": 2
        }
      }
    ]
  },
  "plan": {
    "phases": [
      {
        "phase_id": "P1",
        "name": "Authentication Core",
        "description": "Implement JWT generation, validation, and user login",
        "estimated_tokens": 25000,
        "estimated_duration_hours": 4,
        "stories": [
          {
            "story_id": "S1.1",
            "description": "Implement JWT token generation and validation utilities",
            "acceptance_criteria": [
              "generateToken() creates valid JWT with user payload",
              "verifyToken() correctly validates and decodes tokens",
              "Expired tokens are rejected",
              "Malformed tokens throw appropriate errors"
            ],
            "tasks": [
              {
                "task_id": "T1.1.1",
                "description": "Create auth utility module with JWT functions",
                "type": "code",
                "estimated_tokens": 3000,
                "dependencies": [],
                "artifacts": ["src/utils/auth.js"],
                "validation_rules": ["eslint", "unit-tests"]
              },
              {
                "task_id": "T1.1.2",
                "description": "Write unit tests for JWT utilities",
                "type": "test",
                "estimated_tokens": 2000,
                "dependencies": ["T1.1.1"],
                "artifacts": ["tests/unit/utils/auth.test.js"],
                "validation_rules": ["jest", "coverage>=90%"]
              }
            ]
          }
        ]
      }
    ],
    "decision_records": [
      {
        "id": "ADR-001",
        "title": "JWT vs Session-Based Authentication",
        "decision": "Use JWT with refresh tokens stored in Redis",
        "rationale": "Stateless authentication enables horizontal scaling. Refresh tokens in Redis provide revocation capability while maintaining performance.",
        "alternatives": [
          "Pure session-based: Requires sticky sessions or shared session store",
          "JWT only (no refresh): Cannot revoke tokens until expiry"
        ],
        "consequences": {
          "positive": ["Stateless auth scales easily", "Can revoke access via refresh token blacklist"],
          "negative": ["Redis dependency", "Slightly more complex token flow"],
          "mitigations": ["Redis clustering for HA", "Clear documentation of token flow"]
        },
        "status": "accepted"
      }
    ]
  },
  "state": {
    "current_phase": "P1",
    "current_story": "S1.1",
    "completed_tasks": [],
    "blocked_tasks": [],
    "status": "approved",
    "context_usage": {
      "tokens_used": 12000,
      "tokens_available": 200000,
      "percentage": 6.0
    },
    "validation_results": {
      "lint": {"status": "not_run"},
      "tests": {"status": "not_run"},
      "security": {"status": "not_run"},
      "performance": {"status": "not_run"}
    }
  }
}
```

---

## Context & Token Management

### Token Budget Strategy

**Context Window Thresholds**:
- **< 50%**: Green zone - proceed normally
- **50-70%**: Yellow zone - monitor closely, plan checkpoint soon
- **70-85%**: Orange zone - optimize context (compress summaries, remove stale info)
- **> 85%**: Red zone - MANDATORY checkpoint, request new session

**Context Optimization Techniques**:
1. **Summarization**: Collapse completed phases into concise summaries (â‰¤500 tokens each)
2. **Artifact Registry**: Maintain file â†’ description mapping instead of full file contents
3. **Differential State**: Store only changes since last checkpoint, not full state
4. **External Storage**: Move large artifacts (test data, logs) to files, reference by path
5. **Pruning**: Remove temporary debugging information older than current phase

### Token Budgeting per Task

**Recommended Token Allocation**:
```
Planning Phase: 15-20% of window
  - Requirements analysis: 5-10%
  - Design & architecture decision record: 5-7%
  - Task breakdown: 3-5%

Implementation Phase: 50-60% of window
  - Code generation: 30-40%
  - Test writing: 15-20%
  - Documentation: 5-8%

Validation Phase: 10-15% of window
  - Running tests: 5-8%
  - Security scans: 3-5%
  - Manual review: 2-4%

Buffer/Reserve: 10-15% of window
  - Error recovery: 5-10%
  - Unexpected complexity: 5%
```

### State Checkpointing Protocol

**When to Checkpoint**:
- Context usage reaches 70% threshold
- Completing a major phase (epic or large story)
- Before making risky changes (schema migrations, API breaking changes)
- After encountering and recovering from errors
- Every 4 hours of continuous work

**Checkpoint Artifacts**:
```json
{
  "checkpoint": {
    "id": "CP-20251111-143000",
    "timestamp": "2025-11-11T14:30:00Z",
    "context_snapshot": {
      "tokens_used": 140000,
      "percentage": 70,
      "plan_manifest_path": ".llm_state/plan_manifest_v3.json",
      "phase_summary_path": ".llm_state/phase_summary_p2.md",
      "artifacts_registry_path": ".llm_state/artifacts_v3.json",
      "decision_records_path": "docs/decisions/"
    },
    "resume_instructions": {
      "next_task": "T2.3.1",
      "blockers": [],
      "context_to_load": [
        "plan_manifest",
        "current phase summary",
        "last 3 decision records",
        "validation results"
      ]
    }
  }
}
```

---

## Testing & Verification

### Testing Pyramid

```
        /\
       /  \
      / E2E \     â† 10% (Critical user flows only)
     /------\
    /        \
   /Integration\ â† 20% (API contracts, DB interactions)
  /------------\
 /              \
/  Unit Tests    \ â† 70% (Business logic, utilities)
------------------
```

**Test Coverage Requirements**:
- **Unit Tests**: â‰¥80% line coverage, â‰¥70% branch coverage
- **Integration Tests**: All API endpoints, database operations, external service integrations
- **E2E Tests**: Top 5 user flows (happy path + critical errors)
- **Property-Based Tests**: For complex algorithms, data transformations, parsers

### Validation Pipeline

**Pre-Commit Checks** (automated):
```bash
# Linting
npm run lint        # or: pylint, rubocop, etc.
# Expected: 0 errors, warnings acceptable if documented

# Unit Tests
npm test
# Expected: 100% pass, coverage >= 80%

# Security Scanning
npm audit --audit-level=high
# Expected: 0 high or critical vulnerabilities

# Type Checking (if applicable)
npm run typecheck
# Expected: 0 type errors
```

**Post-Commit Checks** (CI pipeline):
```yaml
# .github/workflows/validate.yml
name: Validation Pipeline
on: [pull_request]
jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      # Install tools conditionally based on project type
      - name: Setup environment
        run: |
          # Detect project type and install tools
          if [ -f "package.json" ]; then
            npm ci
          fi
          if [ -f "requirements.txt" ]; then
            pip install -r requirements.txt
          fi
          if [ -f "Gemfile" ]; then
            bundle install
          fi
      
      - name: Lint
        run: |
          if [ -f "package.json" ] && [ -n "$(npm run | grep lint)" ]; then
            npm run lint
          elif [ -f "requirements.txt" ]; then
            pylint src/ || true
          fi
      
      - name: Test
        run: |
          if [ -f "package.json" ] && [ -n "$(npm run | grep test)" ]; then
            npm test
          elif [ -f "requirements.txt" ]; then
            pytest tests/
          fi
      
      - name: Security Scan
        run: |
          if [ -f "package.json" ]; then
            npm audit --audit-level=high || exit 1
          fi
          if [ -f "requirements.txt" ]; then
            pip-audit || exit 1
          fi
      
      - name: Validate Plan Manifest
        run: |
          if [ -f "plan_manifest.json" ]; then
            python scripts/validate_manifest.py plan_manifest.json
          fi
```

**Decision Record Validation**:
```bash
# Check that decision records follow ADR template
python scripts/validate_decision_records.py docs/decisions/

# Expected output:
# âœ“ All decision records follow template
# âœ“ All decisions have status field
# âœ“ All decisions reference related tasks
```

---

## Multi-Agent Patterns

### Agent Orchestration Models

**1. Sequential Pipeline (Waterfall)**
```
[Planner] â†’ [Implementer] â†’ [Tester] â†’ [Reviewer]
```
- Use when: Clear phases, dependencies are sequential
- Pros: Simple, clear handoffs, easy to debug
- Cons: No parallelization, blocked if one agent stuck

**2. Parallel Specialists**
```
        [Frontend Agent]
[Coordinator] â†’ [Backend Agent]  â†’ [Integration Agent]
        [Data Agent]
```
- Use when: Independent components, can work in parallel
- Pros: Faster completion, specialization benefits
- Cons: Complex coordination, merge conflicts possible

**3. Iterative Refinement**
```
[Draft Agent] â†’ [Critique Agent] â†’ [Refine Agent] â†°
                      â†‘__________________|
```
- Use when: Quality-critical work, multiple perspectives valuable
- Pros: High quality output, catches mistakes
- Cons: Token-intensive, slower

**4. Hierarchical Delegation**
```
      [Architect Agent]
           â†“
    [Technical Lead Agent]
      â†™        â†˜
[Backend Dev] [Frontend Dev]
```
- Use when: Large projects, need coordination across teams
- Pros: Scales well, maintains cohesion
- Cons: Communication overhead, requires good specifications

### Communication Protocol

**Standard Message Format**:
```json
{
  "message_id": "MSG-20251111-143012-001",
  "timestamp": "2025-11-11T14:30:12Z",
  "from_agent": "planner_agent",
  "to_agent": "implementer_agent",
  "message_type": "task_assignment",
  "priority": "high",
  "content": {
    "task_id": "T1.2.3",
    "description": "Implement authentication middleware",
    "context": {
      "plan_manifest": "plan_manifest.json",
      "decision_records": ["ADR-001", "ADR-003"],
      "dependencies": ["T1.2.1", "T1.2.2"]
    },
    "acceptance_criteria": [
      "Middleware validates JWT tokens",
      "Invalid tokens return 401",
      "Unit tests pass with >=80% coverage"
    ],
    "constraints": {
      "max_tokens": 5000,
      "deadline": "2025-11-11T16:00:00Z"
    }
  },
  "requires_response": true,
  "correlation_id": "TASK-T1.2.3"
}
```

**Agent Response Format**:
```json
{
  "message_id": "MSG-20251111-143530-002",
  "timestamp": "2025-11-11T14:35:30Z",
  "from_agent": "implementer_agent",
  "to_agent": "planner_agent",
  "message_type": "task_completion",
  "in_response_to": "MSG-20251111-143012-001",
  "correlation_id": "TASK-T1.2.3",
  "content": {
    "status": "completed",
    "task_id": "T1.2.3",
    "artifacts": [
      "src/middleware/auth.js",
      "tests/unit/middleware/auth.test.js"
    ],
    "validation_results": {
      "lint": {"status": "passed", "errors": 0},
      "tests": {"status": "passed", "coverage": 85.3},
      "security": {"status": "passed", "vulnerabilities": 0}
    },
    "decision_records": ["ADR-005"],
    "notes": "Implemented using JWT strategy. See ADR-005 for rationale on error handling approach.",
    "next_steps": ["T1.2.4 - Integration testing"]
  }
}
```

---

## Advanced Techniques

### Retrieval-Augmented Generation (RAG) Integration

**Document Preprocessing Pipeline**:
```python
# Chunking Strategy
def chunk_document(doc, chunk_size=512, overlap=50):
    """
    Split document with overlap for context preservation.
    Tag each chunk with metadata for safety.
    """
    chunks = []
    for i in range(0, len(doc.tokens), chunk_size - overlap):
        chunk = {
            "content": doc.tokens[i:i+chunk_size],
            "metadata": {
                "source": doc.source,
                "classification": doc.classification,  # e.g., "public", "internal"
                "chunk_id": f"{doc.id}-{i}",
                "retrieval_timestamp": datetime.now().isoformat()
            }
        }
        chunks.append(chunk)
    return chunks
```

**Retrieval with Safety Tags**:
```python
def retrieve_relevant_chunks(query, k=5):
    """
    Retrieve chunks and enforce safety checks.
    """
    results = vector_db.search(query, k=k)
    
    # Validate each result
    safe_results = []
    for result in results:
        # Check classification level
        if result.metadata["classification"] in ALLOWED_CLASSIFICATIONS:
            # Add origin tag for transparency
            result.metadata["origin"] = "RAG retrieval"
            safe_results.append(result)
        else:
            log_security_event("Filtered restricted document", result.metadata)
    
    return safe_results
```

**RAG-Enhanced Prompt Template**:
```markdown
## CONTEXT FROM RETRIEVED DOCUMENTS

**Source Validation**: All retrieved documents verified against allowlist

{{#each retrieved_chunks}}
---
**Document**: {{metadata.source}}
**Classification**: {{metadata.classification}}
**Retrieval Time**: {{metadata.retrieval_timestamp}}

{{content}}
---
{{/each}}

## YOUR TASK

Using the context above (if relevant), [original task instructions]

**Important**: 
- Only use information from sources marked as "{{allowed_classification}}"
- Do not combine information from different classification levels
- If unsure about source reliability, ask for confirmation
```

### Function Calling / Tool Use Patterns

**Tool Registry with Permissions**:
```json
{
  "tools": [
    {
      "name": "execute_query",
      "description": "Execute read-only SQL query on database",
      "parameters": {
        "type": "object",
        "properties": {
          "query": {"type": "string"},
          "database": {"type": "string", "enum": ["analytics", "reporting"]}
        },
        "required": ["query", "database"]
      },
      "permissions": {
        "allowed_operations": ["SELECT"],
        "forbidden_operations": ["INSERT", "UPDATE", "DELETE", "DROP"],
        "requires_confirmation": false,
        "max_rows": 1000
      },
      "validation_schema": "schemas/sql_query_schema.json"
    },
    {
      "name": "deploy_service",
      "description": "Deploy service to specified environment",
      "parameters": {
        "type": "object",
        "properties": {
          "service_name": {"type": "string"},
          "environment": {"type": "string", "enum": ["dev", "staging", "production"]},
          "version": {"type": "string"}
        },
        "required": ["service_name", "environment", "version"]
      },
      "permissions": {
        "requires_confirmation": true,
        "confirmation_message": "Deploy {{service_name}} v{{version}} to {{environment}}?",
        "allowed_environments": ["dev", "staging"],
        "forbidden_environments": ["production"]
      }
    }
  ]
}
```

**Safe Tool Execution Wrapper**:
```python
def execute_tool(tool_name, parameters, config):
    """
    Execute tool with safety checks and validation.
    """
    # 1. Validate tool exists and is allowed
    tool = config.tools.get(tool_name)
    if not tool:
        raise ToolNotFoundError(f"Tool {tool_name} not in registry")
    
    # 2. Validate parameters against JSON schema
    validate(parameters, tool.validation_schema)
    
    # 3. Check permissions
    if tool.requires_confirmation:
        confirmation = get_human_confirmation(
            tool.confirmation_message.format(**parameters)
        )
        if not confirmation:
            raise ToolExecutionDenied("User declined confirmation")
    
    # 4. Apply parameter sanitization
    sanitized_params = sanitize_parameters(parameters, tool.permissions)
    
    # 5. Execute with timeout and error handling
    try:
        result = tool.execute(sanitized_params, timeout=30)
        log_tool_execution(tool_name, sanitized_params, result, "success")
        return result
    except Exception as e:
        log_tool_execution(tool_name, sanitized_params, str(e), "failure")
        raise ToolExecutionError(f"Tool execution failed: {e}")
```

### Prompt Versioning & Regression Testing

**Version Control for Prompts**:
```
prompts/
  v1.0/
    planning_prompt.md
    execution_prompt.md
  v2.0/
    planning_prompt.md
    execution_prompt.md
  v2.1/
    planning_prompt.md
    execution_prompt.md
  v2.2/  â† Current
    planning_prompt.md
    execution_prompt.md
    tool_safety_prompt.md  â† New
```

**Regression Test Suite**:
```python
# tests/prompt_regression/test_planning_v2_2.py

import pytest
from llm_client import LLMClient
from prompt_loader import load_prompt

@pytest.fixture
def client():
    return LLMClient(model="claude-sonnet-4-20250514")

def test_planning_generates_valid_manifest(client):
    """
    Regression test: Planning prompt should generate valid plan_manifest.json
    """
    prompt = load_prompt("prompts/v2.2/planning_prompt.md")
    requirements = load_test_fixture("fixtures/auth_requirements.md")
    
    response = client.generate(prompt + requirements)
    manifest = extract_json(response, "plan_manifest.json")
    
    # Validate against schema v2.2
    validate_manifest(manifest, "schemas/plan_manifest_v2_2.json")
    
    # Check critical fields
    assert "meta" in manifest
    assert "prompt_version" in manifest["meta"]
    assert manifest["meta"]["prompt_version"] == "2.2"
    assert "dependencies" in manifest
    assert "new_packages" in manifest["dependencies"]

def test_planning_includes_decision_records(client):
    """
    Regression test: Planning should include decision records, not raw CoT
    """
    prompt = load_prompt("prompts/v2.2/planning_prompt.md")
    requirements = load_test_fixture("fixtures/complex_requirements.md")
    
    response = client.generate(prompt + requirements)
    manifest = extract_json(response, "plan_manifest.json")
    
    # Should have decision records
    assert "decision_records" in manifest["plan"]
    assert len(manifest["plan"]["decision_records"]) > 0
    
    # Should not contain raw reasoning markers
    assert "chain-of-thought" not in response.lower()
    assert "my reasoning process" not in response.lower()

def test_security_prompts_prevent_injection(client):
    """
    Regression test: Tool execution should reject injection attempts
    """
    prompt = load_prompt("prompts/v2.2/tool_safety_prompt.md")
    
    # Injection attempt
    malicious_input = {
        "query": "SELECT * FROM users; DROP TABLE users; --",
        "database": "analytics"
    }
    
    response = client.generate(prompt + str(malicious_input))
    
    # Should reject or sanitize
    assert "DROP" not in response or "rejected" in response.lower()
    assert "security" in response.lower() or "invalid" in response.lower()
```

---

## Prompt Injection & Tool-Use Safety

### Overview

**Threat Model**: LLM agents with RAG and tool access face unique security risks:
- **Prompt Injection**: Malicious instructions embedded in retrieved documents or user input
- **Tool Misuse**: Unintended or unauthorized tool execution
- **Data Exfiltration**: Sensitive data leaking through tool outputs or generated content
- **Privilege Escalation**: Exploiting tool permissions to access restricted resources

**Defense Strategy**: Defense-in-depth with multiple layers of controls.

### Input Validation & Sanitization

**Prompt Injection Detection**:
```python
# Pattern-based detection
INJECTION_PATTERNS = [
    r"ignore (all )?previous (instructions|commands|prompts)",
    r"disregard (the )?above",
    r"new (instructions|task|objective):",
    r"system (prompt|message|instruction):",
    r"you are now",
    r"forget (everything|all|your instructions)",
    r"<\|im_start\|>",  # Special tokens
    r"<\|im_end\|>",
    r"\[INST\]",
    r"\[/INST\]",
]

def detect_injection_attempt(text):
    """
    Detect potential prompt injection patterns.
    Returns (is_suspicious, matched_patterns)
    """
    matched = []
    for pattern in INJECTION_PATTERNS:
        if re.search(pattern, text, re.IGNORECASE):
            matched.append(pattern)
    
    return len(matched) > 0, matched

def sanitize_user_input(text, context="user_query"):
    """
    Sanitize user input before processing.
    """
    # 1. Detect injection attempts
    is_suspicious, patterns = detect_injection_attempt(text)
    if is_suspicious:
        log_security_event("prompt_injection_attempt", {
            "context": context,
            "patterns": patterns,
            "input_preview": text[:200]
        })
        # Option A: Reject outright
        raise SecurityError("Input contains suspicious patterns")
        
        # Option B: Sanitize (use with caution)
        # for pattern in INJECTION_PATTERNS:
        #     text = re.sub(pattern, "[REDACTED]", text, flags=re.IGNORECASE)
    
    # 2. Remove special tokens
    text = remove_special_tokens(text)
    
    # 3. Normalize whitespace
    text = " ".join(text.split())
    
    return text
```

**RAG Document Validation**:
```python
def validate_rag_document(doc):
    """
    Validate retrieved document before inclusion in context.
    """
    # 1. Check classification level
    if doc.metadata.get("classification") not in ALLOWED_CLASSIFICATIONS:
        raise SecurityError(f"Document classification not allowed: {doc.metadata.get('classification')}")
    
    # 2. Verify source authenticity
    if not verify_document_signature(doc):
        raise SecurityError("Document signature verification failed")
    
    # 3. Scan for injection patterns
    is_suspicious, patterns = detect_injection_attempt(doc.content)
    if is_suspicious:
        log_security_event("suspicious_rag_document", {
            "source": doc.metadata.get("source"),
            "patterns": patterns
        })
        # Option: Filter or tag the document
        doc.metadata["security_warning"] = "Contains suspicious patterns"
    
    # 4. Add origin tags
    doc.metadata["origin"] = "RAG"
    doc.metadata["retrieved_at"] = datetime.now().isoformat()
    doc.metadata["validation_status"] = "passed"
    
    return doc
```

### Tool Execution Safety

**Tool Allowlisting**:
```python
# config/tool_allowlist.json
{
  "allowed_tools": [
    {
      "name": "search_documentation",
      "max_calls_per_session": 10,
      "parameters": {
        "query": {"type": "string", "max_length": 200}
      }
    },
    {
      "name": "execute_safe_query",
      "max_calls_per_session": 5,
      "parameters": {
        "query": {"type": "string", "pattern": "^SELECT .+"},
        "database": {"type": "string", "enum": ["analytics_ro"]}
      },
      "requires_confirmation": true
    }
  ],
  "forbidden_tools": [
    "execute_shell_command",
    "modify_database",
    "delete_file",
    "send_email"  # Unless explicitly required and approved
  ]
}
```

**Schema-Validated Tool Input**:
```python
from jsonschema import validate, ValidationError

def safe_tool_execution(tool_name, parameters, allowlist):
    """
    Execute tool with comprehensive safety checks.
    """
    # 1. Check tool is in allowlist
    tool_config = allowlist.get(tool_name)
    if not tool_config:
        raise ToolNotAllowedError(f"Tool {tool_name} not in allowlist")
    
    # 2. Validate parameters against schema
    try:
        validate(instance=parameters, schema=tool_config["parameters"])
    except ValidationError as e:
        raise InvalidParametersError(f"Parameter validation failed: {e}")
    
    # 3. Check rate limits
    call_count = get_tool_call_count(tool_name, session_id)
    if call_count >= tool_config.get("max_calls_per_session", float('inf')):
        raise RateLimitError(f"Too many calls to {tool_name}")
    
    # 4. Sanitize parameters
    sanitized = sanitize_tool_parameters(parameters)
    
    # 5. Request confirmation if required
    if tool_config.get("requires_confirmation"):
        if not get_human_confirmation(f"Execute {tool_name}({sanitized})?"):
            raise ToolExecutionDenied("User declined")
    
    # 6. Execute with timeout and logging
    try:
        result = execute_with_timeout(tool_name, sanitized, timeout=30)
        log_tool_call(tool_name, sanitized, result, "success")
        increment_tool_call_count(tool_name, session_id)
        return result
    except Exception as e:
        log_tool_call(tool_name, sanitized, str(e), "failure")
        raise ToolExecutionError(f"Execution failed: {e}")
```

**Confirmation Workflow for Destructive Operations**:
```python
def get_human_confirmation(message, context=None):
    """
    Request explicit human confirmation for sensitive operations.
    """
    confirmation_request = {
        "type": "confirmation_required",
        "message": message,
        "context": context,
        "timestamp": datetime.now().isoformat(),
        "timeout_seconds": 300,  # 5 minutes
        "required_action": "user_must_respond"
    }
    
    # Send to UI or notification channel
    response = send_confirmation_request(confirmation_request)
    
    # Log confirmation result
    log_confirmation_event(message, response, context)
    
    return response.get("approved", False)

# Example usage in agent code
if tool.is_destructive:
    confirmed = get_human_confirmation(
        f"About to delete {len(items)} items. Confirm?",
        context={"tool": "delete_items", "count": len(items)}
    )
    if not confirmed:
        return {"status": "cancelled", "reason": "User declined confirmation"}
```

### RAG Source Verification

**Origin Tagging**:
```python
def tag_rag_sources(chunks, classification_level="internal"):
    """
    Add metadata tags to all RAG chunks for traceability.
    """
    tagged_chunks = []
    for chunk in chunks:
        chunk["metadata"]["origin"] = "RAG"
        chunk["metadata"]["classification"] = classification_level
        chunk["metadata"]["retrieval_timestamp"] = datetime.now().isoformat()
        chunk["metadata"]["verified"] = verify_source_authenticity(chunk)
        
        # Add watermark for internal tracking
        chunk["metadata"]["tracking_id"] = generate_tracking_id()
        
        tagged_chunks.append(chunk)
    
    return tagged_chunks
```

**Context Separation**:
```markdown
## SYSTEM INSTRUCTIONS
[Your core prompt and rules]

## RETRIEVED CONTEXT
**IMPORTANT**: The following content is from external sources. Validate claims before using.

{{#each rag_chunks}}
---
SOURCE: {{metadata.source}}
CLASSIFICATION: {{metadata.classification}}
VERIFIED: {{metadata.verified}}
RETRIEVED: {{metadata.retrieval_timestamp}}

{{content}}
---
{{/each}}

## USER QUERY
{{user_query}}

## RESPONSE REQUIREMENTS
- Only use information from sources marked as VERIFIED: true
- Do not combine information from different classification levels
- Cite sources for all factual claims
- If source information conflicts with training, prefer source but note the discrepancy
```

### Output Sanitization

**Sensitive Data Filtering**:
```python
import re

PII_PATTERNS = {
    "email": r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b",
    "ssn": r"\b\d{3}-\d{2}-\d{4}\b",
    "credit_card": r"\b\d{4}[- ]?\d{4}[- ]?\d{4}[- ]?\d{4}\b",
    "api_key": r"\b[A-Za-z0-9]{32,}\b",  # Common API key format
    "jwt": r"eyJ[A-Za-z0-9_-]+\.eyJ[A-Za-z0-9_-]+\.[A-Za-z0-9_-]+",
}

def sanitize_output(text, classification_level="public"):
    """
    Remove or redact sensitive information from output.
    """
    sanitized = text
    redactions = []
    
    # Apply classification-specific rules
    if classification_level in ["public", "internal"]:
        for pattern_name, pattern in PII_PATTERNS.items():
            matches = re.findall(pattern, sanitized)
            if matches:
                redactions.append({
                    "type": pattern_name,
                    "count": len(matches)
                })
                sanitized = re.sub(pattern, f"[REDACTED-{pattern_name.upper()}]", sanitized)
    
    # Log redactions for audit
    if redactions:
        log_security_event("output_sanitization", {
            "redactions": redactions,
            "classification": classification_level
        })
    
    return sanitized
```

### Security Checklist for RAG & Tool Use

**Pre-Deployment**:
- [ ] Tool allowlist defined and reviewed
- [ ] All tools have JSON schema validation
- [ ] Destructive tools require human confirmation
- [ ] Rate limits configured per tool
- [ ] RAG sources verified and classified
- [ ] Injection detection patterns tested
- [ ] Output sanitization rules configured
- [ ] Audit logging enabled for all tool calls

**Per-Session**:
- [ ] Validate user input for injection attempts
- [ ] Verify RAG document classifications
- [ ] Check tool execution against allowlist
- [ ] Request confirmation for sensitive operations
- [ ] Sanitize outputs before returning to user
- [ ] Log all security events

**Post-Incident**:
- [ ] Review audit logs for suspicious activity
- [ ] Update injection patterns based on new attacks
- [ ] Refine tool permissions
- [ ] Update RAG source validation rules
- [ ] Document lessons learned in security decision records

---

## Security & Compliance

### Security-First Design Principles

1. **Least Privilege**: Grant minimum necessary permissions
2. **Defense in Depth**: Multiple security layers
3. **Zero Trust**: Verify everything, assume breach
4. **Privacy by Design**: Minimize data collection and retention
5. **Secure by Default**: Opt-in for sensitive features, opt-out for security

### Secrets Management

**NEVER hardcode secrets**:
```python
# âŒ BAD
api_key = "sk-1234567890abcdef"
db_password = "admin123"

# âœ… GOOD
import os
from secret_manager import get_secret

api_key = os.getenv("API_KEY")  # From environment
db_password = get_secret("database/password")  # From secret manager
```

**Secrets Detection in CI**:
```bash
# Use tools like gitleaks, trufflehog
- name: Scan for secrets
  run: |
    gitleaks detect --source . --verbose --redact

# Expected: No secrets found
```

### Data Classification & Handling

**Classification Levels**:
- **Public**: Can be freely shared (documentation, marketing content)
- **Internal**: Company confidential (business plans, internal tools)
- **Confidential**: Sensitive business data (financial records, customer PII)
- **Restricted**: Highly sensitive (credentials, encryption keys, health records)

**Handling Requirements by Classification**:
```markdown
| Classification | Encryption at Rest | Encryption in Transit | Access Logging | Retention Limit |
|----------------|--------------------|-----------------------|----------------|-----------------|
| Public         | Optional           | Optional              | No             | Indefinite      |
| Internal       | Recommended        | Required              | Yes            | 7 years         |
| Confidential   | Required (AES-256) | Required (TLS 1.3)    | Yes            | 7 years         |
| Restricted     | Required (AES-256) | Required (TLS 1.3)    | Yes (detailed) | 90 days*        |

* Unless regulatory requirements dictate longer retention
```

### Regulatory Compliance Checklists

**GDPR Compliance**:
- [ ] **Data Minimization**: Collect only necessary data
- [ ] **Purpose Limitation**: Use data only for stated purposes
- [ ] **Consent Management**: Obtain and track user consent
- [ ] **Right to Access**: Provide data export mechanism
- [ ] **Right to Erasure**: Implement data deletion
- [ ] **Right to Rectification**: Allow data correction
- [ ] **Data Portability**: Export in machine-readable format (JSON)
- [ ] **Privacy by Design**: Build privacy into system architecture
- [ ] **Data Protection Impact Assessment (DPIA)**: Complete for high-risk processing
- [ ] **Breach Notification**: Alert users within 72 hours of breach

**HIPAA Compliance** (for healthcare data):
- [ ] **Access Controls**: Role-based access to PHI
- [ ] **Audit Logging**: Log all PHI access
- [ ] **Encryption**: Encrypt PHI at rest and in transit
- [ ] **Business Associate Agreements (BAA)**: Signed with all vendors
- [ ] **Breach Notification**: Alert users and HHS within 60 days
- [ ] **Minimum Necessary**: Access only necessary PHI
- [ ] **Training**: Annual security training for all staff

### Dependency Security

**Vulnerability Scanning**:
```bash
# Node.js
npm audit --audit-level=high
npm audit fix --audit-level=high

# Python
pip-audit
safety check --json

# Ruby
bundle audit check --update

# Rust
cargo audit
```

**Dependency Review Policy**:
```markdown
## New Dependency Checklist

Before adding any new dependency:
- [ ] Actively maintained (commit in last 6 months)
- [ ] No known critical/high vulnerabilities
- [ ] Compatible license (MIT, Apache 2.0, BSD preferred)
- [ ] Reasonable number of transitive dependencies (< 50)
- [ ] Used by reputable projects (> 1000 weekly downloads)
- [ ] Documented alternatives considered (in decision record)
- [ ] Approved by security team (for new direct dependencies)

Document rationale in plan_manifest.json under dependencies.new_packages.
```

---

## Observability & Metrics

### Logging Best Practices (v2.2 Updated)

**CRITICAL PRIVACY RULE**: Never log raw internal reasoning, chain-of-thought, or scratchpad content.

**Log Levels**:
- **ERROR**: System failures, unhandled exceptions
- **WARN**: Recoverable errors, degraded performance, deprecated features
- **INFO**: Key business events, decision outcomes (from decision records)
- **DEBUG**: Decision record references, validation results (never raw reasoning)
- **TRACE**: Detailed flow (never enabled in production)

**Structured Logging Format**:
```json
{
  "timestamp": "2025-11-11T14:30:12.123Z",
  "level": "INFO",
  "logger": "agent.planner",
  "message": "Decision record created for authentication approach",
  "context": {
    "task_id": "T1.2.1",
    "decision_id": "ADR-001",
    "decision_status": "accepted",
    "alternatives_count": 3
  },
  "correlation_id": "EPIC-AUTH-001",
  "session_id": "sess_20251111_143000",
  "user_id": "user_12345",  # Hash if PII
  "agent_version": "2.2"
}
```

**What to Log** âœ…:
- Decision record IDs and summaries
- Validation results (pass/fail, metrics)
- Performance metrics (duration, token usage)
- Error summaries and recovery actions
- Security events (attempted injections, denied tool executions)
- Tool execution attempts and outcomes (with sanitized parameters)

**What NOT to Log** âŒ:
- Raw chain-of-thought reasoning
- Internal agent scratchpad content
- Personally Identifiable Information (PII) without hashing
- Secrets, API keys, tokens, passwords
- Full file contents (log diffs or summaries only)
- Raw user input without sanitization

**Redaction Rules**:
```python
def redact_logs(log_entry):
    """
    Redact sensitive information before persisting logs.
    """
    # Redact PII
    if "user_email" in log_entry:
        log_entry["user_email_hash"] = hash_pii(log_entry["user_email"])
        del log_entry["user_email"]
    
    # Redact secrets
    for key in list(log_entry.keys()):
        if any(secret_word in key.lower() for secret_word in ["password", "token", "key", "secret"]):
            log_entry[key] = "[REDACTED]"
    
    # Remove raw reasoning if accidentally included
    if "reasoning" in log_entry or "chain_of_thought" in log_entry:
        del log_entry["reasoning"]
        log_entry["_warning"] = "Reasoning removed for privacy"
    
    return log_entry
```

### Metrics & KPIs

**Performance Metrics**:
```python
# Track these per task
metrics = {
    "task_duration_seconds": 45.3,
    "tokens_used": {
        "input": 5000,
        "output": 2000,
        "cached": 3000  # If caching used
    },
    "cost_usd": 0.15,
    "validation_results": {
        "lint_errors": 0,
        "test_failures": 0,
        "security_issues": 0,
        "coverage_pct": 85.3
    },
    "context_efficiency": {
        "context_used_pct": 45,
        "checkpoint_count": 0
    }
}
```

**Quality Metrics**:
```python
# Track these per session/epic
quality_metrics = {
    "planning_accuracy": {
        "estimated_hours": 8,
        "actual_hours": 9.5,
        "variance_pct": 18.75
    },
    "first_time_success_rate": 0.87,  # % tasks that pass validation first try
    "self_correction_rate": 0.13,  # % tasks requiring self-correction
    "human_intervention_rate": 0.05,  # % tasks requiring human help
    "decision_record_quality": {
        "records_created": 5,
        "records_with_alternatives": 5,
        "records_with_consequences": 5
    }
}
```

**Dashboard Layout**:
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    AGENT PERFORMANCE DASHBOARD                 â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Current Session: sess_20251111_143000                         â•‘
â•‘ Epic: AUTH-001 | Phase: P2 (Implementation)                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                â•‘
â•‘  TASK METRICS                          QUALITY GATES          â•‘
â•‘  â”œâ”€ Completed: 12/15 (80%)             â”œâ”€ Lint: âœ… 0 errors  â•‘
â•‘  â”œâ”€ In Progress: 2                     â”œâ”€ Tests: âœ… 100% passâ•‘
â•‘  â”œâ”€ Blocked: 1                         â”œâ”€ Security: âœ… Clean â•‘
â•‘  â””â”€ Avg Duration: 35 min               â””â”€ Coverage: âœ… 85.3% â•‘
â•‘                                                                â•‘
â•‘  COST & EFFICIENCY                     CONTEXT USAGE          â•‘
â•‘  â”œâ”€ Total Cost: $2.45                  â”œâ”€ Current: 45%       â•‘
â•‘  â”œâ”€ Avg per Task: $0.20                â”œâ”€ Peak: 68%          â•‘
â•‘  â”œâ”€ Tokens Used: 245K                  â”œâ”€ Checkpoints: 1     â•‘
â•‘  â””â”€ Cache Hit Rate: 85%                â””â”€ Status: ðŸŸ¢ Healthyâ•‘
â•‘                                                                â•‘
â•‘  DECISION RECORDS                      ERRORS & WARNINGS      â•‘
â•‘  â”œâ”€ Created: 5                         â”œâ”€ Errors: 2 (resolved)â•‘
â•‘  â”œâ”€ Accepted: 4                        â”œâ”€ Warnings: 1        â•‘
â•‘  â”œâ”€ Proposed: 1                        â”œâ”€ Injections: 0      â•‘
â•‘  â””â”€ Avg Alternatives: 2.8              â””â”€ Tool Denials: 0    â•‘
â•‘                                                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Audit Trail Requirements

**Decision Records Storage**:
```
docs/
  decisions/
    README.md              # Index and navigation
    ADR-001-auth-approach.md
    ADR-002-database-choice.md
    ADR-003-caching-strategy.md
    [timestamp]_[decision_id].md
```

**Audit Log Structure**:
```json
{
  "audit_id": "AUDIT-20251111-143012",
  "timestamp": "2025-11-11T14:30:12Z",
  "event_type": "decision_made",
  "agent_id": "planner_agent_v2.2",
  "user_id": "user_12345_hash",
  "session_id": "sess_20251111_143000",
  "task_id": "T1.2.1",
  "decision_record": {
    "id": "ADR-001",
    "title": "JWT vs Session Authentication",
    "decision": "Use JWT with Redis refresh tokens",
    "status": "accepted"
  },
  "context": {
    "alternatives_considered": 3,
    "estimated_impact": "medium",
    "requires_approval": false
  },
  "retention": {
    "classification": "internal",
    "retain_until": "2032-11-11",  # 7 years for business records
    "deletion_policy": "manual_review_required"
  }
}
```

---

## Error Handling & Recovery

### Error Classification

**Error Severity Levels**:
1. **Critical**: System cannot continue, data loss possible, security breach
2. **High**: Feature broken, cannot complete task, blocking issue
3. **Medium**: Degraded functionality, workaround available
4. **Low**: Minor issue, doesn't impact primary functionality

### Decision Trees

#### 1. Validation Failure Decision Tree

```
Validation Failed
     â”‚
     â”œâ”€ Tests Failed?
     â”‚   â”œâ”€ 1-3 tests â†’ Self-correct (1 attempt max)
     â”‚   â”‚                â””â”€ Success? Continue : Report to human
     â”‚   â””â”€ >3 tests â†’ Report to human immediately
     â”‚
     â”œâ”€ Lint Errors?
     â”‚   â”œâ”€ <5 errors â†’ Self-correct
     â”‚   â”‚                â””â”€ Success? Continue : Report to human
     â”‚   â””â”€ â‰¥5 errors â†’ Report to human
     â”‚
     â”œâ”€ Security Issues?
     â”‚   â”œâ”€ Critical/High â†’ HALT + Report immediately
     â”‚   â””â”€ Medium/Low â†’ Report + Propose fix
     â”‚
     â””â”€ Performance Regression?
         â”œâ”€ <10% â†’ Log + Continue (monitor)
         â””â”€ â‰¥10% â†’ Report + Propose optimization
```

#### 2. Agent Selection Decision Tree

```
Select Agent for Task
     â”‚
     â”œâ”€ Task Type?
     â”‚   â”œâ”€ Planning â†’ Planner Agent
     â”‚   â”œâ”€ Coding â†’ Implementation Agent
     â”‚   â”œâ”€ Testing â†’ Test Agent
     â”‚   â””â”€ Review â†’ Review Agent
     â”‚
     â”œâ”€ Complexity?
     â”‚   â”œâ”€ Simple â†’ Single specialized agent
     â”‚   â”œâ”€ Complex â†’ Multi-agent (coordinator + specialists)
     â”‚   â””â”€ Uncertain â†’ Start with Planner, delegate after analysis
     â”‚
     â””â”€ Context Size?
         â”œâ”€ <50% â†’ Proceed with standard agent
         â”œâ”€ 50-70% â†’ Use agent with summarization capability
         â””â”€ >70% â†’ Checkpoint + Fresh agent in new session
```

#### 3. Rollback Decision Tree

```
Issue Detected in Production
     â”‚
     â”œâ”€ Severity?
     â”‚   â”œâ”€ Critical (data loss, security, outage)
     â”‚   â”‚   â””â”€ IMMEDIATE ROLLBACK
     â”‚   â”‚       â””â”€ Execute: kubectl rollout undo deployment/[service]
     â”‚   â”‚
     â”‚   â”œâ”€ High (feature broken, significant impact)
     â”‚   â”‚   â””â”€ Quick fix available? (<5 min)
     â”‚   â”‚       â”œâ”€ Yes â†’ Hotfix + Fast-forward deploy
     â”‚   â”‚       â””â”€ No â†’ ROLLBACK
     â”‚   â”‚
     â”‚   â””â”€ Medium/Low (minor issues, workarounds exist)
     â”‚       â””â”€ Fix in next release, monitor
     â”‚
     â””â”€ Rollback Successful?
         â”œâ”€ Yes â†’ Incident review + Root cause analysis
         â””â”€ No â†’ Escalate to on-call + Manual intervention
```

#### 4. Context Management Decision Tree

```
Check Context Usage
     â”‚
     â”œâ”€ Current Usage?
     â”‚   â”œâ”€ <50% â†’ ðŸŸ¢ Green Zone
     â”‚   â”‚   â””â”€ Proceed normally
     â”‚   â”‚
     â”‚   â”œâ”€ 50-70% â†’ ðŸŸ¡ Yellow Zone
     â”‚   â”‚   â””â”€ Monitor closely
     â”‚   â”‚       â””â”€ Plan checkpoint within next 2-3 tasks
     â”‚   â”‚
     â”‚   â”œâ”€ 70-85% â†’ ðŸŸ  Orange Zone
     â”‚   â”‚   â””â”€ Optimize first
     â”‚   â”‚       â”œâ”€ Summarize completed phases (â‰¤500 tokens each)
     â”‚   â”‚       â”œâ”€ Move large artifacts to external files
     â”‚   â”‚       â”œâ”€ Compress decision records (keep titles + decisions only)
     â”‚   â”‚       â””â”€ Then: Continue if <75%, else Checkpoint
     â”‚   â”‚
     â”‚   â””â”€ >85% â†’ ðŸ”´ Red Zone
     â”‚       â””â”€ MANDATORY CHECKPOINT
     â”‚           â””â”€ Save state + Request new session
     â”‚
     â””â”€ After Optimization?
         â”œâ”€ Still >75% â†’ Checkpoint anyway
         â””â”€ <75% â†’ Continue with close monitoring
```

### Error Recovery Patterns

**Pattern 1: Retry with Exponential Backoff**:
```python
import time
from functools import wraps

def retry_with_backoff(max_attempts=3, base_delay=1, backoff_factor=2):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            delay = base_delay
            for attempt in range(1, max_attempts + 1):
                try:
                    return func(*args, **kwargs)
                except RetryableError as e:
                    if attempt == max_attempts:
                        log_error(f"Max retries ({max_attempts}) exceeded", e)
                        raise
                    
                    log_warning(f"Attempt {attempt} failed, retrying in {delay}s", e)
                    time.sleep(delay)
                    delay *= backoff_factor
        return wrapper
    return decorator

@retry_with_backoff(max_attempts=3)
def generate_code(task):
    # Might fail due to API rate limits, transient errors
    return llm_client.generate(task)
```

**Pattern 2: Graceful Degradation**:
```python
def execute_with_fallback(primary_func, fallback_func, *args, **kwargs):
    """
    Try primary function, fall back to simpler alternative if it fails.
    """
    try:
        return primary_func(*args, **kwargs)
    except Exception as e:
        log_warning(f"Primary function failed: {e}, using fallback")
        return fallback_func(*args, **kwargs)

# Example: Fall back to simpler code generation if advanced fails
result = execute_with_fallback(
    generate_optimized_code,
    generate_basic_code,
    task_spec
)
```

**Pattern 3: Circuit Breaker**:
```python
class CircuitBreaker:
    def __init__(self, failure_threshold=5, timeout=60):
        self.failure_count = 0
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.last_failure_time = None
        self.state = "CLOSED"  # CLOSED, OPEN, HALF_OPEN
    
    def call(self, func, *args, **kwargs):
        if self.state == "OPEN":
            if time.time() - self.last_failure_time > self.timeout:
                self.state = "HALF_OPEN"
            else:
                raise CircuitBreakerOpenError("Circuit breaker is OPEN")
        
        try:
            result = func(*args, **kwargs)
            if self.state == "HALF_OPEN":
                self.state = "CLOSED"
                self.failure_count = 0
            return result
        except Exception as e:
            self.failure_count += 1
            self.last_failure_time = time.time()
            
            if self.failure_count >= self.failure_threshold:
                self.state = "OPEN"
                log_error("Circuit breaker opened due to repeated failures")
            raise

# Usage
breaker = CircuitBreaker()
result = breaker.call(external_api_call, params)
```

---

## Templates & Examples

### Planning Prompt Template (Pasteable)

```markdown
# LLM Development Agent - Planning Phase

You are a Planning Agent (v2.2) with authority to create implementation plans.
You CANNOT execute code or make changes without an approved plan.

## OBJECTIVE

[Paste your requirements here]

## CONTEXT & CONSTRAINTS

**Technical Stack**:
- Languages: [e.g., Python 3.11, TypeScript 5.0]
- Frameworks: [e.g., FastAPI, React 18]
- Infrastructure: [e.g., AWS ECS, PostgreSQL 15]
- Repository: [GitHub URL]

**Standards**:
- Test Coverage: â‰¥80%
- Linter: [eslint/pylint/etc.] with [config]
- Security: Zero critical/high vulnerabilities
- Performance: p95 latency â‰¤ [X]ms

**Security**:
- Data Classification: [public/internal/confidential/restricted]
- Encryption: [at-rest/in-transit requirements]
- Regulatory: [GDPR/HIPAA/etc.]

## DELIVERABLES REQUIRED

1. **Decision Records** (docs/decisions/ADR-XXX-*.md):
   - Approach selection with alternatives
   - Trade-off analysis
   - Risk assessment
   - NEVER include raw chain-of-thought

2. **Plan Manifest** (plan_manifest.json):
   - Must validate against schema v2.2
   - Include meta.prompt_version = "2.2"
   - Include dependencies.new_packages with security scan status

3. **Implementation Plan** (docs/implementation_plan.md):
   - Phased breakdown (Epic â†’ Stories â†’ Tasks)
   - Estimated token budget per phase
   - Validation gates

## REASONING REQUIREMENTS

1. State your understanding of the requirements
2. Ask 2-3 clarifying questions if anything is ambiguous
3. Create decision records (ADR) for key architectural choices
4. Propose 2-3 approaches with trade-offs
5. Recommend approach with clear rationale

**CRITICAL**: Output decision records only, never raw reasoning or chain-of-thought.

## APPROVAL GATE

After generating plan, WAIT for explicit approval:
```
APPROVED_BY: [name]
DATE: [YYYY-MM-DD]
```

Only proceed to execution after approval received.
```

### Execution Prompt Template (Pasteable)

```markdown
# LLM Development Agent - Execution Phase

You are an Implementation Agent (v2.2) with authority to generate code following approved plans.
You CANNOT deviate from the approved plan without creating a new decision record.

## LOAD CONTEXT

Load these artifacts:
- Approved plan_manifest.json (check meta.approved_by is set)
- Relevant decision records (docs/decisions/)
- Current task from plan.phases[].stories[].tasks[]

## CURRENT TASK

**Task ID**: [from plan_manifest]
**Description**: [from plan_manifest]
**Type**: [code/test/doc/config]
**Estimated Tokens**: [budget]

## EXECUTION REQUIREMENTS

1. **Code Generation**:
   - Follow style guide from plan
   - Use idempotent, configuration-driven patterns
   - No hardcoded values (use config files)
   - Include inline documentation

2. **Test Generation** (MANDATORY):
   - Unit tests for all functions
   - Integration tests for API endpoints
   - Target coverage: â‰¥80%
   - Include edge cases and error scenarios

3. **Documentation**:
   - Update README if needed
   - Add inline comments for complex logic
   - Update API docs if endpoints changed

4. **Decision Records**:
   - If implementation deviates from plan, create ADR
   - If you make significant design choice, create ADR
   - Include rationale and alternatives

## VALIDATION PIPELINE

After generating code, run:
```bash
# Linter
[your-linter-command]

# Tests
[your-test-command]

# Security
npm audit --audit-level=high  # or equivalent

# Coverage check
[your-coverage-command]
```

## SELF-VALIDATION

Before marking task complete:
- [ ] All acceptance criteria met
- [ ] Tests pass with â‰¥80% coverage
- [ ] Linter reports 0 errors
- [ ] Security scan shows 0 critical/high issues
- [ ] Documentation updated
- [ ] Decision records created for any deviations

## FAILURE HANDLING

If validation fails:
- 1-3 test failures â†’ Self-correct (1 attempt only)
- >3 test failures â†’ Report to human
- Security critical/high â†’ HALT + Report immediately
- Other failures â†’ Use appropriate decision tree (see guide)

## STATE UPDATE

After completion:
1. Update plan_manifest.json state.completed_tasks
2. Create phase summary if phase complete
3. Commit changes with proper message format:
   ```
   [EPIC-ID] type: short â€” detail
   
   - Changes made
   - Tests added
   - Refs: ADR-XXX (if applicable)
   
   Rollback: [rollback instructions]
   ```
```

### Before/After Example

**âŒ BAD (Unstructured Prompt)**:
```
Hey, can you build a login system with JWT tokens? Make it secure and fast. 
Thanks!
```

**Problems**:
- No clear objective or success criteria
- No technical constraints specified
- No planning phase
- No verification gates
- No deliverable formats specified

---

**âœ… GOOD (Structured Prompt following v2.2)**:

```markdown
# Planning Request: JWT Authentication System

## 1. AGENT IDENTITY
You are a Planning Agent (v2.2) for software development.
Authority: Create implementation plans, decision records, task breakdowns
Restrictions: Cannot execute code without approved plan

## 2. OBJECTIVE

**Primary Goal**: Implement secure JWT-based authentication with refresh tokens

**User Stories**:
- As a user, I want to log in with email/password and receive a JWT
- As a user, I want my session to persist securely across browser restarts
- As a user, I want to log out and have my session immediately invalidated

**Success Criteria**:
- Login endpoint accepts credentials and returns valid JWT
- Refresh token mechanism works correctly
- Logout invalidates refresh tokens
- Zero critical/high security vulnerabilities
- p95 latency â‰¤ 100ms

## 3. CONTEXT & CONSTRAINTS

**Technical Stack**:
- Backend: Node.js 20 with Express 4.18
- Database: PostgreSQL 15
- Cache: Redis 7 (for token blacklist)
- Repository: https://github.com/company/api

**Standards**:
- Linter: ESLint with Airbnb config
- Test Coverage: â‰¥80%
- Security: OWASP Top 10 compliant
- Performance: p95 latency â‰¤100ms, 1000 RPS

**Security**:
- Data Classification: Confidential (passwords, tokens)
- Encryption: TLS 1.3 in transit, AES-256 at rest
- Regulatory: GDPR, SOC2
- Forbidden: MD5/SHA1 hashing, storing plain text passwords

**Dependency Policy**:
- Approved registries: npmjs.com
- New dependencies require security scan
- License: MIT/Apache 2.0/BSD only

## 4. REASONING REQUIREMENTS

Before planning:
1. Confirm your understanding of JWT vs session auth
2. Create decision record (ADR) comparing:
   - JWT only (no refresh tokens)
   - JWT + refresh tokens in DB
   - JWT + refresh tokens in Redis
3. Evaluate trade-offs: scalability, security, complexity
4. Recommend approach with rationale
5. Document risks (token theft, replay attacks) and mitigations

## 5. DELIVERABLES

**Decision Records** (docs/decisions/):
- ADR-001: Authentication approach selection
- ADR-002: Password hashing algorithm choice
- ADR-003: Token expiry and refresh strategy

**Plan Manifest** (plan_manifest.json):
- Schema v2.2 compliant
- meta.prompt_version = "2.2"
- dependencies.new_packages with justifications
- Phased breakdown with token estimates

**Implementation Plan** (docs/implementation_plan.md):
- Phase 1: Auth utilities (JWT, hashing)
- Phase 2: Login/logout endpoints
- Phase 3: Refresh token flow
- Phase 4: Integration testing

## 6. APPROVAL GATE

After generating plan, output:
```
AWAITING APPROVAL
To proceed to execution, provide:
APPROVED_BY: [your name]
DATE: [YYYY-MM-DD]
```
```

**Result**: The agent will produce structured decision records, a valid plan_manifest.json (v2.2 schema), and a detailed implementation plan, then wait for approval before proceeding.

---

## Specialized Task Patterns

### Frontend UI Feature

**Pre-Planning Checklist**:
- [ ] Mockups or wireframes available
- [ ] Accessibility requirements defined (WCAG level)
- [ ] Browser support matrix documented
- [ ] Design system components identified
- [ ] State management approach selected

**Planning Template**:
```markdown
## Frontend Feature: [Feature Name]

**Component Hierarchy**:
```
[PageComponent]
  â”œâ”€ [ContainerComponent]
  â”‚   â”œâ”€ [PresentationalComponent1]
  â”‚   â””â”€ [PresentationalComponent2]
  â””â”€ [SharedComponent]
```

**State Management**:
- Local State: [useState, component-specific]
- Global State: [Redux/Context for user auth, theme]
- Server State: [React Query for API data]

**Accessibility**:
- ARIA labels for interactive elements
- Keyboard navigation (Tab, Enter, Escape)
- Screen reader announcements for dynamic content
- Focus management for modals/dialogs

**Testing Strategy**:
- Unit: Component logic with Jest + React Testing Library
- Integration: User flows with React Testing Library
- E2E: Critical paths with Playwright
- Visual Regression: Chromatic or Percy

**Decision Records**:
- ADR-NNN: State management approach (local vs global)
- ADR-NNN: Styling solution (CSS-in-JS vs Tailwind vs CSS Modules)
```

### API Endpoint Development

**Pre-Planning Checklist**:
- [ ] API contract defined (OpenAPI/Swagger spec)
- [ ] Authentication/authorization requirements clear
- [ ] Rate limiting strategy defined
- [ ] Error response formats standardized
- [ ] Monitoring and alerting configured

**Planning Template**:
```markdown
## API Endpoint: [Method] /api/v1/[resource]

**Request Specification**:
```typescript
interface RequestBody {
  field1: string;  // Required, max 100 chars
  field2?: number; // Optional, range 0-1000
}
```

**Response Specification**:
```typescript
// Success (200)
interface SuccessResponse {
  data: ResourceType;
  meta: { timestamp: string };
}

// Error (4xx/5xx)
interface ErrorResponse {
  error: {
    code: string;  // e.g., "INVALID_INPUT"
    message: string;
    details?: object;
  };
}
```

**Validation Rules**:
- Input: JSON schema validation with Joi/Yup
- Authentication: JWT required, scopes: [read:resource]
- Rate Limiting: 100 req/min per user
- Idempotency: Idempotency-Key header for POST/PUT

**Decision Records**:
- ADR-NNN: Error response format standardization
- ADR-NNN: Pagination strategy (cursor vs offset)
- ADR-NNN: Versioning approach (URL vs header)

**Testing Strategy**:
- Unit: Business logic with mocked dependencies
- Integration: Full request/response cycle with test DB
- Contract: OpenAPI spec validation
- Load: Target throughput and latency validation
```

### Data Pipeline / ETL Job

**Pre-Planning Checklist**:
- [ ] Data sources identified and access verified
- [ ] Data volume estimates documented
- [ ] Transformation rules defined
- [ ] Error handling and retry strategy defined
- [ ] Monitoring and alerting configured

**Planning Template**:
```markdown
## Data Pipeline: [Pipeline Name]

**Data Flow**:
```
[Source] â†’ [Extract] â†’ [Transform] â†’ [Load] â†’ [Target]
  â†“          â†“            â†“           â†“         â†“
[S3]     [Lambda]    [Glue Job]  [Lambda]  [Redshift]
```

**Orchestration**:
- Tool: [Airflow/Step Functions/Prefect]
- Schedule: [cron expression or event-driven]
- Retry Policy: [max attempts, backoff strategy]

**Data Quality**:
- Schema validation: [Great Expectations/Soda]
- Null checks: [critical fields must be non-null]
- Range checks: [numeric fields within expected bounds]
- Freshness: [data not older than X hours]

**Error Handling**:
- Transient errors: Retry with exponential backoff (3 attempts)
- Data quality failures: Log + Alert + Skip record
- Source unavailable: Alert + Pause pipeline

**Decision Records**:
- ADR-NNN: Batch vs streaming processing
- ADR-NNN: Data deduplication strategy
- ADR-NNN: Late-arriving data handling

**Testing Strategy**:
- Unit: Transformation logic with sample data
- Integration: End-to-end with test data sources
- Data Quality: Validation rules with edge cases
- Performance: Target throughput validation
```

### Infrastructure as Code (IaC)

**Pre-Planning Checklist**:
- [ ] Cloud provider and region selected
- [ ] Cost estimates reviewed and approved
- [ ] Security groups and network topology defined
- [ ] Disaster recovery requirements documented
- [ ] Compliance requirements reviewed

**Planning Template**:
```markdown
## Infrastructure: [Service/Component Name]

**Architecture**:
```
[Load Balancer]
     â†“
[Auto Scaling Group]
  â”œâ”€ [EC2 Instance 1]
  â””â”€ [EC2 Instance 2]
     â†“
[RDS Primary] â†â†’ [RDS Standby]
```

**Resource Specifications**:
- Compute: [instance type, count, scaling rules]
- Storage: [type, size, IOPS, backup schedule]
- Network: [VPC, subnets, security groups]
- Monitoring: [CloudWatch alarms, log retention]

**Cost Estimates**:
- Monthly: $[amount] (baseline) + $[amount] per 1000 requests
- Cost allocation tags: [project, environment, owner]

**Security**:
- Network: Private subnets for compute, public for LB only
- Encryption: KMS for EBS volumes, TLS for data in transit
- Access: IAM roles with least privilege, no root access
- Secrets: AWS Secrets Manager, rotated every 90 days

**Decision Records**:
- ADR-NNN: IaC tool selection (Terraform vs CloudFormation)
- ADR-NNN: Multi-region vs single-region deployment
- ADR-NNN: RDS vs Aurora vs DynamoDB

**Testing Strategy**:
- Validation: `terraform plan` / `cfn-lint`
- Integration: Deploy to dev environment
- Security: `tfsec` / `checkov` for misconfiguration scanning
- Cost: `infracost` for cost impact analysis
```

### Refactoring / Technical Debt

**Pre-Planning Checklist**:
- [ ] Current state documented (pain points, metrics)
- [ ] Target state defined (desired improvements)
- [ ] Success metrics identified (performance, maintainability)
- [ ] Migration path planned (incremental vs big bang)
- [ ] Rollback strategy defined

**Planning Template**:
```markdown
## Refactoring: [Component/Module Name]

**Current Problems**:
- [Problem 1]: [Impact and metrics]
- [Problem 2]: [Impact and metrics]

**Target State**:
- [Improvement 1]: [Expected benefit]
- [Improvement 2]: [Expected benefit]

**Migration Strategy**:
- Approach: [Strangler Fig / Branch by Abstraction / Big Bang]
- Phases:
  1. [Phase 1]: [description, scope]
  2. [Phase 2]: [description, scope]
- Feature Flags: [for incremental rollout]
- Rollback: [strategy if issues found]

**Risk Assessment**:
- Risk: Breaking existing functionality
  - Mitigation: Comprehensive regression testing, feature flags
- Risk: Performance degradation
  - Mitigation: Load testing before/after, gradual rollout

**Decision Records**:
- ADR-NNN: Refactoring approach selection
- ADR-NNN: Pattern/architecture target state

**Testing Strategy**:
- Regression: Full test suite must pass before and after
- Performance: Baseline metrics, target improvements
- Characterization: Tests to capture current behavior
```

### Bug Fix

**Pre-Planning Checklist**:
- [ ] Bug reproduced and documented
- [ ] Root cause identified
- [ ] Impact assessed (users affected, severity)
- [ ] Fix approach reviewed (patch vs deeper fix)

**Planning Template**:
```markdown
## Bug Fix: [Bug ID] - [Brief Description]

**Problem Statement**:
[Concise description of bug and impact]

**Reproduction Steps**:
1. [Step 1]
2. [Step 2]
3. [Expected vs Actual behavior]

**Root Cause**:
[Technical explanation of what's causing the bug]

**Fix Approach**:
- Immediate Fix: [Quick patch to stop bleeding]
- Long-Term Fix: [Deeper fix to prevent recurrence]

**Decision Record**:
- ADR-NNN: Patch vs comprehensive fix (if applicable)

**Testing Strategy**:
- Regression: Verify bug is fixed
- Unit: Test covering the bug scenario
- Related: Test similar code paths

**Rollout Plan**:
- Severity: [Critical/High/Medium/Low]
- Deployment: [Hotfix / Next release]
- Monitoring: [Specific metrics to watch post-deploy]
```

---

## Cost Modeling & Budgeting

### Pricing Model Configuration (v2.2 Updated)

**CRITICAL**: Never hard-code model names or pricing. Use configuration files that can be updated as providers change pricing.

**Pricing Configuration** (`config/llm_pricing.json`):
```json
{
  "models": {
    "claude-sonnet-4-20250514": {
      "provider": "anthropic",
      "input_per_mtok": 3.00,
      "output_per_mtok": 15.00,
      "cache_write_per_mtok": 3.75,
      "cache_read_per_mtok": 0.30,
      "context_window": 200000,
      "supports_caching": true
    },
    "claude-opus-4-20250514": {
      "provider": "anthropic",
      "input_per_mtok": 15.00,
      "output_per_mtok": 75.00,
      "cache_write_per_mtok": 18.75,
      "cache_read_per_mtok": 1.50,
      "context_window": 200000,
      "supports_caching": true
    },
    "gpt-4-turbo": {
      "provider": "openai",
      "input_per_mtok": 10.00,
      "output_per_mtok": 30.00,
      "cache_write_per_mtok": null,
      "cache_read_per_mtok": null,
      "context_window": 128000,
      "supports_caching": false
    }
  },
  "last_updated": "2025-11-11",
  "currency": "USD",
  "notes": "Prices are per million tokens (Mtok). Update this file when providers change pricing."
}
```

### Cost Calculation Functions (v2.2 Fixed)

**Load Pricing Configuration**:
```python
import json

def load_pricing_config(config_path="config/llm_pricing.json"):
    """
    Load pricing configuration from file.
    """
    with open(config_path) as f:
        return json.load(f)

def get_model_pricing(model_name, config):
    """
    Get pricing for a specific model.
    Raises ValueError if model not found.
    """
    if model_name not in config["models"]:
        available = ", ".join(config["models"].keys())
        raise ValueError(
            f"Model '{model_name}' not found in pricing config. "
            f"Available models: {available}"
        )
    return config["models"][model_name]
```

**First Call Cost (with cache write)**:
```python
def estimate_first_call_cost(input_tokens, output_tokens, model_name, config):
    """
    Estimate cost for FIRST call (writes to cache).
    
    Args:
        input_tokens: Number of input tokens
        output_tokens: Number of output tokens
        model_name: Model identifier (e.g., "claude-sonnet-4-20250514")
        config: Loaded pricing configuration dict
    
    Returns:
        dict with cost breakdown
    """
    pricing = get_model_pricing(model_name, config)
    
    # First call: input tokens written to cache (if supported)
    if pricing["supports_caching"]:
        input_cost = (input_tokens / 1_000_000) * pricing["cache_write_per_mtok"]
        cache_info = "cache write"
    else:
        input_cost = (input_tokens / 1_000_000) * pricing["input_per_mtok"]
        cache_info = "no caching"
    
    output_cost = (output_tokens / 1_000_000) * pricing["output_per_mtok"]
    total_cost = input_cost + output_cost
    
    return {
        "input_tokens": input_tokens,
        "output_tokens": output_tokens,
        "input_cost": round(input_cost, 4),
        "output_cost": round(output_cost, 4),
        "total_cost": round(total_cost, 4),
        "model": model_name,
        "cache_info": cache_info
    }

# Example usage
config = load_pricing_config()
cost = estimate_first_call_cost(
    input_tokens=50000,
    output_tokens=2000,
    model_name="claude-sonnet-4-20250514",
    config=config
)
print(f"First call cost: ${cost['total_cost']:.4f}")
# Output: First call cost: $0.2175 (cache write)
```

**Cached Call Cost (with cache read)**:
```python
def estimate_cached_call_cost(
    cached_tokens,
    new_input_tokens,
    output_tokens,
    model_name,
    config
):
    """
    Estimate cost for SUBSEQUENT calls (reads from cache).
    
    Args:
        cached_tokens: Number of tokens read from cache
        new_input_tokens: Number of new input tokens (not cached)
        output_tokens: Number of output tokens
        model_name: Model identifier
        config: Loaded pricing configuration dict
    
    Returns:
        dict with cost breakdown
    """
    pricing = get_model_pricing(model_name, config)
    
    if not pricing["supports_caching"]:
        # No caching: all tokens charged at input rate
        input_cost = ((cached_tokens + new_input_tokens) / 1_000_000) * pricing["input_per_mtok"]
        cache_info = "no caching available"
        cache_savings = 0
    else:
        # Cache read cost (much cheaper)
        cache_cost = (cached_tokens / 1_000_000) * pricing["cache_read_per_mtok"]
        
        # New input cost (normal rate)
        new_input_cost = (new_input_tokens / 1_000_000) * pricing["input_per_mtok"]
        
        input_cost = cache_cost + new_input_cost
        
        # Calculate savings vs non-cached
        non_cached_cost = ((cached_tokens + new_input_tokens) / 1_000_000) * pricing["input_per_mtok"]
        cache_savings = non_cached_cost - input_cost
        cache_info = f"cache read ({cached_tokens} tokens)"
    
    output_cost = (output_tokens / 1_000_000) * pricing["output_per_mtok"]
    total_cost = input_cost + output_cost
    
    return {
        "cached_tokens": cached_tokens,
        "new_input_tokens": new_input_tokens,
        "output_tokens": output_tokens,
        "input_cost": round(input_cost, 4),
        "output_cost": round(output_cost, 4),
        "total_cost": round(total_cost, 4),
        "cache_savings": round(cache_savings, 4),
        "model": model_name,
        "cache_info": cache_info
    }

# Example usage
cost = estimate_cached_call_cost(
    cached_tokens=48000,  # Most of prompt cached
    new_input_tokens=2000,  # Only new user query
    output_tokens=2000,
    model_name="claude-sonnet-4-20250514",
    config=config
)
print(f"Cached call cost: ${cost['total_cost']:.4f}")
print(f"Cache savings: ${cost['cache_savings']:.4f}")
# Output: 
# Cached call cost: $0.0504 (77% cheaper!)
# Cache savings: $0.1560
```

### Cost Optimization Strategies

**1. Use Prompt Caching Effectively**:
```python
def design_cacheable_prompt(base_prompt, user_query):
    """
    Structure prompt to maximize cache reuse.
    
    Cache-friendly structure:
    [System prompt - 20K tokens] â† Cached
    [Examples - 15K tokens]      â† Cached
    [Context - 10K tokens]       â† Cached
    ---
    [User query - 500 tokens]    â† Not cached (changes each time)
    """
    cacheable_prefix = f"{base_prompt}\n\n{examples}\n\n{context}\n\n---\n\n"
    full_prompt = f"{cacheable_prefix}User Query: {user_query}"
    
    return {
        "prompt": full_prompt,
        "cacheable_tokens": len(tokenize(cacheable_prefix)),
        "variable_tokens": len(tokenize(user_query))
    }
```

**2. Choose Right Model for Task**:
```python
def select_cost_effective_model(task_complexity, config):
    """
    Choose appropriate model based on task complexity.
    """
    if task_complexity == "simple":
        # Use cheaper model for simple tasks
        return "claude-sonnet-4-20250514"  # $0.003/1K input tokens
    elif task_complexity == "medium":
        return "claude-sonnet-4-20250514"  # Good balance
    else:  # complex
        return "claude-opus-4-20250514"    # Most capable, but expensive
```

**3. Incremental Generation**:
```python
def generate_incrementally(tasks, model_name, config):
    """
    Generate code incrementally instead of all at once.
    Reduces wasted tokens if early tasks fail.
    """
    results = []
    for task in tasks:
        result = generate_code(task, model_name)
        
        # Validate before continuing
        if not validate(result):
            return {
                "status": "failed",
                "completed": results,
                "failed_at": task,
                "cost_saved": estimate_remaining_cost(tasks[len(results):], model_name, config)
            }
        
        results.append(result)
    
    return {"status": "success", "results": results}
```

### Budget Allocation Framework

**Project Budget Template**:
```json
{
  "project": "AUTH-001",
  "budget": {
    "total_usd": 50.00,
    "allocated": {
      "planning": 10.00,       // 20%
      "implementation": 25.00,  // 50%
      "testing": 7.50,         // 15%
      "refinement": 7.50       // 15%
    },
    "spent": {
      "planning": 8.50,
      "implementation": 0.00,
      "testing": 0.00,
      "refinement": 0.00
    },
    "remaining": 41.50
  },
  "cost_per_phase": {
    "planning": [
      {"task": "requirements_analysis", "estimated": 5.00, "actual": 4.25},
      {"task": "decision_records", "estimated": 5.00, "actual": 4.25}
    ]
  },
  "alerts": [
    {
      "threshold": "80%",
      "action": "notify_lead"
    },
    {
      "threshold": "95%",
      "action": "halt_and_review"
    }
  ]
}
```

**Cost Tracking Script**:
```python
def track_task_cost(task_id, actual_cost, budget_file="budget.json"):
    """
    Track actual cost against budget and alert if thresholds exceeded.
    """
    with open(budget_file) as f:
        budget = json.load(f)
    
    # Update spent amount
    phase = get_phase_for_task(task_id)
    budget["spent"][phase] += actual_cost
    
    # Calculate remaining
    budget["remaining"] = budget["budget"]["total_usd"] - sum(budget["spent"].values())
    
    # Check thresholds
    spent_pct = (sum(budget["spent"].values()) / budget["budget"]["total_usd"]) * 100
    
    for alert in budget["alerts"]:
        threshold_pct = int(alert["threshold"].rstrip("%"))
        if spent_pct >= threshold_pct:
            trigger_alert(alert["action"], spent_pct, budget)
    
    # Save updated budget
    with open(budget_file, "w") as f:
        json.dump(budget, f, indent=2)
    
    return budget

# Example usage
budget = track_task_cost("T1.2.1", 0.25)
print(f"Remaining budget: ${budget['remaining']:.2f}")
```

### Determinism & Sampling Guidance (v2.2 New)

**Sampling Parameters Standard**:
```python
# Deterministic settings (for testing, regression tests, reproducibility)
DETERMINISTIC_PARAMS = {
    "temperature": 0.0,    # No randomness
    "top_p": 1.0,          # Consider all tokens
    "seed": 42,            # Fixed seed (if supported by provider)
    "frequency_penalty": 0.0,
    "presence_penalty": 0.0
}

# Balanced settings (for general development tasks)
BALANCED_PARAMS = {
    "temperature": 0.3,    # Some creativity, but consistent
    "top_p": 0.9,
    "seed": None,          # Allow variation
    "frequency_penalty": 0.0,
    "presence_penalty": 0.0
}

# Creative settings (for brainstorming, multiple alternatives)
CREATIVE_PARAMS = {
    "temperature": 0.7,    # More variation
    "top_p": 0.9,
    "seed": None,
    "frequency_penalty": 0.3,   # Discourage repetition
    "presence_penalty": 0.3
}
```

**When to Use Each Setting**:
```python
def select_sampling_params(task_type):
    """
    Select appropriate sampling parameters based on task type.
    """
    task_params = {
        # Reproducibility required
        "test_generation": DETERMINISTIC_PARAMS,
        "regression_test": DETERMINISTIC_PARAMS,
        "ci_validation": DETERMINISTIC_PARAMS,
        
        # Consistency preferred
        "code_generation": BALANCED_PARAMS,
        "documentation": BALANCED_PARAMS,
        "bug_fix": BALANCED_PARAMS,
        
        # Variation desired
        "brainstorming": CREATIVE_PARAMS,
        "design_alternatives": CREATIVE_PARAMS,
        "creative_content": CREATIVE_PARAMS
    }
    
    return task_params.get(task_type, BALANCED_PARAMS)

# Example: Configure client for specific task
client = LLMClient(
    model="claude-sonnet-4-20250514",
    **select_sampling_params("code_generation")
)
```

**Testing Reproducibility**:
```python
def test_deterministic_output():
    """
    Verify that deterministic settings produce identical outputs.
    """
    prompt = "Generate a function to sort an array in Python"
    
    client = LLMClient(model="claude-sonnet-4-20250514", **DETERMINISTIC_PARAMS)
    
    # Generate twice
    output1 = client.generate(prompt)
    output2 = client.generate(prompt)
    
    # Should be identical
    assert output1 == output2, "Deterministic outputs should match"
```

---

## Production Operations

### Deployment Checklist

**Pre-Deployment**:
- [ ] All tests pass (unit, integration, e2e)
- [ ] Security scan shows zero critical/high vulnerabilities
- [ ] Performance tests meet targets
- [ ] Documentation updated
- [ ] Rollback plan documented and tested
- [ ] Monitoring and alerting configured
- [ ] Feature flags configured (if applicable)
- [ ] Stakeholders notified

**During Deployment**:
- [ ] Deploy to staging first
- [ ] Run smoke tests on staging
- [ ] Deploy to production (canary or blue/green)
- [ ] Monitor key metrics (errors, latency, throughput)
- [ ] Verify feature flags working correctly

**Post-Deployment**:
- [ ] Monitor for 1 hour after deployment
- [ ] Check error rates and logs
- [ ] Validate business metrics
- [ ] Close deployment ticket
- [ ] Update runbook if needed

### Monitoring & Alerting

**Key Metrics to Monitor**:
```yaml
# metrics.yaml
metrics:
  - name: error_rate
    threshold: 1%  # Alert if >1% of requests fail
    window: 5m
    severity: high
  
  - name: p95_latency_ms
    threshold: 200  # Alert if p95 >200ms
    window: 5m
    severity: medium
  
  - name: throughput_rps
    threshold: 100  # Alert if <100 RPS (expected baseline)
    window: 5m
    severity: low
  
  - name: security_events
    threshold: 1  # Alert on any security event
    window: 1m
    severity: critical
```

**Alert Response Runbook**:
```markdown
## High Error Rate Alert

**Symptoms**: Error rate >1% for 5 minutes

**Immediate Actions**:
1. Check logs for error patterns
2. Identify affected endpoints/features
3. If widespread: Initiate rollback
4. If isolated: Disable feature flag

**Investigation**:
1. Review recent deployments
2. Check dependency health
3. Review error stack traces
4. Check resource utilization (CPU, memory)

**Resolution**:
1. Fix root cause
2. Deploy hotfix OR re-enable after rollback fix
3. Verify metrics return to normal
4. Create postmortem

**Escalation**:
- After 15 min: Page on-call engineer
- After 30 min: Page engineering lead
- After 1 hour: Page CTO
```

### Incident Response

**Severity Levels**:
- **SEV-1 (Critical)**: System down, data loss, security breach
  - Response time: Immediate
  - Update frequency: Every 15 minutes
- **SEV-2 (High)**: Major feature broken, significant user impact
  - Response time: Within 30 minutes
  - Update frequency: Every hour
- **SEV-3 (Medium)**: Minor feature broken, workaround available
  - Response time: Within 4 hours
  - Update frequency: Daily
- **SEV-4 (Low)**: Cosmetic issue, no functional impact
  - Response time: Next business day
  - Update frequency: As needed

**Incident Response Template**:
```markdown
# INCIDENT: [Title]

**Status**: [Investigating / Identified / Monitoring / Resolved]
**Severity**: [SEV-1/2/3/4]
**Started**: [ISO-8601 timestamp]
**Last Updated**: [ISO-8601 timestamp]

## Impact
- Users affected: [number or percentage]
- Services affected: [list]
- Business impact: [revenue, reputation, etc.]

## Timeline
- [HH:MM] Event detected
- [HH:MM] Investigation started
- [HH:MM] Root cause identified
- [HH:MM] Fix deployed
- [HH:MM] Verified resolved

## Root Cause
[Technical explanation]

## Resolution
[Actions taken to resolve]

## Prevention
[Actions to prevent recurrence]

## Follow-up Tasks
- [ ] Update monitoring
- [ ] Add regression tests
- [ ] Update runbooks
- [ ] Postmortem meeting
```

---

## Checklist & Governance

### Complete Enforcement Validation (v2.2 Updated)

**Enhanced Validation Script** (`scripts/validate_manifest.py`):
```python
#!/usr/bin/env python3
"""
LLM Agent Plan Manifest Validator v2.2

Validates plan_manifest.json against schema v2.2 and enforces quality standards.
Updated for v2.2 with:
- Fixed schema alignment (meta.prompt_version, dependencies.new_packages)
- Determinism checks
- Security validation improvements
"""

import json
import sys
from pathlib import Path
from datetime import datetime
from jsonschema import validate, ValidationError, Draft7Validator
from typing import Dict, List, Tuple

# Schema v2.2 (corrected structure)
MANIFEST_SCHEMA_V2_2 = {
    "$schema": "http://json-schema.org/draft-07/schema#",
    "version": "2.2",
    "type": "object",
    "required": ["meta", "objective", "constraints", "dependencies", "plan", "state"],
    "properties": {
        "meta": {
            "type": "object",
            "required": ["epic_id", "created_at", "prompt_version"],
            "properties": {
                "epic_id": {"type": "string", "pattern": "^[A-Z]+-[0-9]+$"},
                "created_at": {"type": "string", "format": "date-time"},
                "updated_at": {"type": "string", "format": "date-time"},
                "prompt_version": {"type": "string", "pattern": "^[0-9]+\\.[0-9]+$"},
                "agent_version": {"type": "string"},
                "approved_by": {"type": "string"},
                "approved_at": {"type": "string", "format": "date-time"}
            }
        },
        "dependencies": {
            "type": "object",
            "properties": {
                "existing_packages": {"type": "array"},
                "new_packages": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "required": ["name", "version", "justification"],
                        "properties": {
                            "name": {"type": "string"},
                            "version": {"type": "string"},
                            "justification": {"type": "string"},
                            "alternatives_considered": {"type": "array"},
                            "security_scan_status": {
                                "type": "string",
                                "enum": ["pending", "passed", "failed"]
                            }
                        }
                    }
                },
                "tools_and_resources": {"type": "array"}
            }
        },
        "plan": {
            "type": "object",
            "required": ["phases"],
            "properties": {
                "phases": {"type": "array", "minItems": 1},
                "decision_records": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "required": ["id", "title", "decision", "status"],
                        "properties": {
                            "id": {"type": "string"},
                            "title": {"type": "string"},
                            "decision": {"type": "string"},
                            "status": {
                                "type": "string",
                                "enum": ["proposed", "accepted", "deprecated", "superseded"]
                            }
                        }
                    }
                }
            }
        },
        # ... other properties from schema
    }
}

class ManifestValidator:
    def __init__(self, manifest_path: str):
        self.manifest_path = Path(manifest_path)
        self.manifest = self._load_manifest()
        self.errors: List[str] = []
        self.warnings: List[str] = []
    
    def _load_manifest(self) -> Dict:
        """Load and parse manifest file."""
        try:
            with open(self.manifest_path) as f:
                return json.load(f)
        except json.JSONDecodeError as e:
            print(f"âŒ Invalid JSON: {e}")
            sys.exit(1)
        except FileNotFoundError:
            print(f"âŒ File not found: {self.manifest_path}")
            sys.exit(1)
    
    def validate_schema(self) -> bool:
        """Validate against JSON schema v2.2."""
        print("ðŸ” Validating against schema v2.2...")
        try:
            validate(instance=self.manifest, schema=MANIFEST_SCHEMA_V2_2)
            print("âœ… Schema validation passed")
            return True
        except ValidationError as e:
            self.errors.append(f"Schema validation failed: {e.message}")
            return False
    
    def validate_version(self) -> bool:
        """Verify prompt_version is 2.2."""
        print("ðŸ” Checking prompt version...")
        expected = "2.2"
        actual = self.manifest.get("meta", {}).get("prompt_version")
        
        if actual != expected:
            self.errors.append(
                f"Version mismatch: expected {expected}, got {actual}"
            )
            return False
        
        print(f"âœ… Prompt version: {actual}")
        return True
    
    def validate_dependencies(self) -> bool:
        """Validate dependency declarations."""
        print("ðŸ” Validating dependencies...")
        deps = self.manifest.get("dependencies", {})
        new_packages = deps.get("new_packages", [])
        
        issues = []
        for pkg in new_packages:
            # Check required fields
            if not pkg.get("justification"):
                issues.append(f"{pkg['name']}: Missing justification")
            
            if not pkg.get("alternatives_considered"):
                self.warnings.append(
                    f"{pkg['name']}: No alternatives documented"
                )
            
            # Check security scan
            scan_status = pkg.get("security_scan_status")
            if scan_status == "failed":
                issues.append(f"{pkg['name']}: Failed security scan")
            elif scan_status == "pending":
                self.warnings.append(
                    f"{pkg['name']}: Security scan pending"
                )
        
        if issues:
            self.errors.extend(issues)
            return False
        
        print(f"âœ… Dependencies validated ({len(new_packages)} new packages)")
        return True
    
    def validate_decision_records(self) -> bool:
        """Validate decision records exist and are complete."""
        print("ðŸ” Validating decision records...")
        records = self.manifest.get("plan", {}).get("decision_records", [])
        
        if not records:
            self.warnings.append("No decision records found")
            return True
        
        for record in records:
            if not record.get("rationale"):
                self.warnings.append(
                    f"{record['id']}: Missing rationale"
                )
            if not record.get("alternatives"):
                self.warnings.append(
                    f"{record['id']}: No alternatives documented"
                )
        
        print(f"âœ… Decision records validated ({len(records)} records)")
        return True
    
    def validate_no_hardcoded_secrets(self) -> bool:
        """Scan for potential hardcoded secrets."""
        print("ðŸ” Scanning for hardcoded secrets...")
        
        manifest_str = json.dumps(self.manifest, indent=2)
        
        secret_patterns = [
            ("password", r"password.*[:=].*[\w]{8,}"),
            ("api_key", r"api[_-]?key.*[:=].*[\w]{20,}"),
            ("token", r"token.*[:=].*[\w]{20,}"),
            ("secret", r"secret.*[:=].*[\w]{20,}"),
        ]
        
        import re
        found_secrets = []
        for name, pattern in secret_patterns:
            if re.search(pattern, manifest_str, re.IGNORECASE):
                found_secrets.append(name)
        
        if found_secrets:
            self.errors.append(
                f"Potential hardcoded secrets found: {', '.join(found_secrets)}"
            )
            return False
        
        print("âœ… No hardcoded secrets detected")
        return True
    
    def validate_all(self) -> Tuple[bool, List[str], List[str]]:
        """Run all validations."""
        print("\n" + "="*60)
        print("LLM Agent Manifest Validator v2.2")
        print("="*60 + "\n")
        
        checks = [
            self.validate_schema(),
            self.validate_version(),
            self.validate_dependencies(),
            self.validate_decision_records(),
            self.validate_no_hardcoded_secrets(),
        ]
        
        success = all(checks)
        
        print("\n" + "="*60)
        if success:
            print("âœ… ALL VALIDATIONS PASSED")
        else:
            print("âŒ VALIDATION FAILED")
        
        if self.errors:
            print("\nðŸš¨ ERRORS:")
            for error in self.errors:
                print(f"  â€¢ {error}")
        
        if self.warnings:
            print("\nâš ï¸  WARNINGS:")
            for warning in self.warnings:
                print(f"  â€¢ {warning}")
        
        print("="*60 + "\n")
        
        return success, self.errors, self.warnings

def main():
    if len(sys.argv) < 2:
        print("Usage: python validate_manifest.py <plan_manifest.json>")
        sys.exit(1)
    
    validator = ManifestValidator(sys.argv[1])
    success, errors, warnings = validator.validate_all()
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()
```

**Enhanced CI/CD Integration** (`.github/workflows/validate.yml`):
```yaml
name: Plan Manifest Validation v2.2
on:
  pull_request:
    paths:
      - 'plan_manifest.json'
      - 'src/**'
      - 'tests/**'

jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for diff
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install validation tools
        run: |
          pip install jsonschema pyyaml
      
      # Conditionally setup Node.js only if package.json exists
      - name: Setup Node.js (if needed)
        if: hashFiles('package.json') != ''
        uses: actions/setup-node@v4
        with:
          node-version: '20'
      
      - name: Install dependencies (if needed)
        if: hashFiles('package.json') != ''
        run: npm ci
      
      # Conditionally setup Python environment if requirements.txt exists
      - name: Setup Python deps (if needed)
        if: hashFiles('requirements.txt') != ''
        run: pip install -r requirements.txt
      
      - name: Validate Plan Manifest
        run: |
          if [ -f "plan_manifest.json" ]; then
            python scripts/validate_manifest.py plan_manifest.json
          else
            echo "â„¹ï¸  No plan_manifest.json found, skipping validation"
          fi
      
      - name: Lint (ecosystem-aware)
        run: |
          if [ -f "package.json" ] && npm run | grep -q "lint"; then
            npm run lint
          elif [ -f "requirements.txt" ]; then
            pylint src/ --exit-zero || echo "âš ï¸  Pylint warnings found"
          elif [ -f "Gemfile" ]; then
            bundle exec rubocop || echo "âš ï¸  Rubocop warnings found"
          else
            echo "â„¹ï¸  No linting configured"
          fi
      
      - name: Run Tests (ecosystem-aware)
        run: |
          if [ -f "package.json" ] && npm run | grep -q "test"; then
            npm test
          elif [ -f "requirements.txt" ]; then
            pytest tests/ --cov=src --cov-report=term-missing
          elif [ -f "Gemfile" ]; then
            bundle exec rspec
          else
            echo "â„¹ï¸  No tests configured"
          fi
      
      - name: Security Scan (ecosystem-aware)
        run: |
          if [ -f "package.json" ]; then
            npm audit --audit-level=high || exit 1
          fi
          if [ -f "requirements.txt" ]; then
            pip-audit || exit 1
          fi
          if [ -f "Gemfile" ]; then
            bundle audit check --update || exit 1
          fi
      
      - name: Check for Decision Records
        run: |
          if [ -f "plan_manifest.json" ]; then
            record_count=$(jq '.plan.decision_records | length' plan_manifest.json)
            if [ "$record_count" -eq 0 ]; then
              echo "âš ï¸  No decision records found in plan"
            else
              echo "âœ… Found $record_count decision records"
            fi
          fi
      
      - name: Validate Decision Record Files
        run: |
          if [ -d "docs/decisions" ]; then
            for file in docs/decisions/ADR-*.md; do
              if [ -f "$file" ]; then
                # Check ADR has required sections
                if ! grep -q "## Decision" "$file"; then
                  echo "âŒ $file missing Decision section"
                  exit 1
                fi
                if ! grep -q "## Consequences" "$file"; then
                  echo "âš ï¸  $file missing Consequences section"
                fi
              fi
            done
            echo "âœ… Decision record files validated"
          fi
```

### Pre-Commit Hook

**Install hook** (`.git/hooks/pre-commit`):
```bash
#!/bin/bash
# LLM Agent Pre-Commit Hook v2.2

echo "ðŸ” Running pre-commit validations..."

# 1. Validate plan manifest if changed
if git diff --cached --name-only | grep -q "plan_manifest.json"; then
    echo "Validating plan_manifest.json..."
    python scripts/validate_manifest.py plan_manifest.json
    if [ $? -ne 0 ]; then
        echo "âŒ Plan manifest validation failed"
        exit 1
    fi
fi

# 2. Check for hardcoded secrets
echo "Scanning for secrets..."
if command -v gitleaks &> /dev/null; then
    gitleaks protect --staged --verbose --redact
    if [ $? -ne 0 ]; then
        echo "âŒ Secrets detected in staged files"
        exit 1
    fi
else
    echo "âš ï¸  gitleaks not installed, skipping secret scan"
fi

# 3. Lint staged files (ecosystem-aware)
if [ -f "package.json" ] && npm run | grep -q "lint"; then
    npm run lint:staged 2>/dev/null || npm run lint
elif command -v pylint &> /dev/null && [ -f "requirements.txt" ]; then
    pylint $(git diff --cached --name-only --diff-filter=ACM | grep '\.py$')
fi

# 4. Run fast tests
echo "Running fast tests..."
if [ -f "package.json" ] && npm run | grep -q "test:fast"; then
    npm run test:fast
elif [ -f "requirements.txt" ]; then
    pytest tests/ -m "not slow" --maxfail=1
fi

echo "âœ… Pre-commit checks passed"
```

---

## Appendix

### Quick Reference Card

*(Same as earlier in document - included here for completeness)*

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         LLM DEVELOPMENT AGENT - QUICK REFERENCE v2.2                 â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£

PHASE 1: PLANNING (ALWAYS FIRST)
  â–¡ Load requirements + constraints
  â–¡ Ask clarifying questions (2-3 max)
  â–¡ Create decision records (ADR) - NOT raw chain-of-thought
  â–¡ Generate plan (human .md + machine .json v2.2)
  â–¡ WAIT for human approval: APPROVED_BY: <name> <date>
  
PHASE 2: EXECUTION (AFTER APPROVAL ONLY)
  â–¡ Load plan_manifest.json (validate meta.prompt_version = "2.2")
  â–¡ Reference decision records for approach rationale
  â–¡ Generate code + tests
  â–¡ Run validations: lint + tests + security
  â–¡ Create PR with rollback steps
  â–¡ Update plan_manifest.json

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MANDATORY CHECKS BEFORE CODING
  âœ“ Requirements clear (if not, ask questions)
  âœ“ Plan approved (check plan_manifest.json approved_by field)
  âœ“ Dependencies scanned (security)
  âœ“ Context <70% (if >70%, checkpoint)
  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

QUALITY GATES (ALL MUST PASS)
  âœ“ Unit tests: â‰¥80% coverage, all passing
  âœ“ Linter: 0 errors
  âœ“ Security: 0 critical/high vulnerabilities
  âœ“ Performance: meets targets (p95 latency, throughput)
  âœ“ Docs: complete (inline, README, API docs)
  âœ“ Decision records: created for all significant choices

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

VALIDATION FAILED? DECISION TREE
  Tests failed:
    1-3 tests â†’ Self-correct (1 attempt only)
    >3 tests â†’ Report to human
  
  Lint errors:
    <5 errors â†’ Self-correct
    â‰¥5 errors â†’ Report to human
  
  Security issues:
    Critical/High â†’ HALT + Report immediately
    Medium/Low â†’ Report + propose fix
  
  Performance regression:
    <10% â†’ Log + continue
    â‰¥10% â†’ Report + propose optimization

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CONTEXT WINDOW MANAGEMENT
  <50%: âœ… Proceed normally
  50-70%: âš ï¸ Monitor closely, plan checkpoint
  70-85%: âš ï¸ Optimize first (compress, summarize)
  >85%: ðŸ›‘ CHECKPOINT NOW, request new session

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

NEVER DO THESE (CRITICAL RULES)
  âœ— Generate code without approved plan
  âœ— Output raw chain-of-thought or internal reasoning
  âœ— Add dependencies without security scan + alternatives
  âœ— Hardcode secrets/credentials in code
  âœ— Include PII in logs or examples
  âœ— Skip tests ("I'll add them later" = âœ—)
  âœ— Make breaking changes without migration path
  âœ— Execute external API calls without approval
  âœ— Log internal reasoning or scratchpad content

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SECURITY CONTROLS (v2.2 NEW)
  âœ“ Validate inputs for prompt injection patterns
  âœ“ Use tool allowlists, not denylists
  âœ“ Require schema validation for all tool inputs
  âœ“ Tag RAG documents with origin/classification
  âœ“ Get human confirmation for destructive operations
  âœ“ Sanitize outputs before returning to user
  âœ“ Never log sensitive data, PII, or raw reasoning

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

WHEN TO ASK FOR HELP
  â€¢ Requirements ambiguous (ask 2-3 clarifying questions)
  â€¢ Validation failed twice (self-correction limit reached)
  â€¢ Security critical/high vulnerabilities found
  â€¢ Performance regression >10%
  â€¢ Context window >85% (checkpoint needed)
  â€¢ Uncertain about approach (explain uncertainty in ADR)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

COMMIT MESSAGE FORMAT
  [Epic-ID] type: short description â€” detail
  
  Types: feat, fix, refactor, test, docs, chore, perf, security, ci
  
  Example:
  [E-001] feat: add JWT auth endpoint â€” POST /auth/login with rate limiting

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

FILES TO MAINTAIN
  plan_manifest.json      - Full state (schema v2.2)
  phase_summary.md        - Concise summary (â‰¤500 tokens)
  docs/decisions/ADR-*.md - Decision records (no raw CoT)
  artifacts_registry.json - Task â†’ file mapping
  .llm_audit/[date]/[task].json - Audit trail

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

COST OPTIMIZATION TIPS
  â€¢ Use caching for stable context (up to 90% savings)
  â€¢ Generate incrementally, not all-at-once
  â€¢ Choose right model for task complexity
  â€¢ Use deterministic settings (temp=0) for testing
  â€¢ Monitor cost_per_task metric
  â€¢ Target: $0.10-0.30 per task

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DETERMINISM SETTINGS (v2.2 NEW)
  Testing/CI: temperature=0.0, top_p=1.0, seed=42
  Development: temperature=0.3, top_p=0.9
  Creative: temperature=0.7, top_p=0.9

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EMERGENCY CONTACTS
  On-Call: [PagerDuty/phone]
  Escalation: [Manager/Lead]
  Rollback: kubectl rollout undo deployment/[service]

â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Version: 2.2 | Updated: 2025-11-11
Key Changes: Prompt injection controls, fixed schema, accurate cost modeling
```

---

### Schema Comparison: v2.1 vs v2.2

**Key Fixes in v2.2**:

1. **prompt_version location**: Moved from root to `meta.prompt_version`
2. **Dependencies structure**: Added `dependencies.new_packages` at correct level
3. **Decision records**: Mandatory in `plan.decision_records`
4. **Type consistency**: All numeric thresholds use `integer` or `number` consistently

```diff
{
  "meta": {
    "epic_id": "AUTH-001",
    "created_at": "2025-11-11T10:00:00Z",
+   "prompt_version": "2.2",  // MOVED HERE from root level
    "agent_version": "claude-sonnet-4-20250514"
  },
- "prompt_version": "2.1",  // REMOVED from root
  "dependencies": {
    "existing_packages": [...],
+   "new_packages": [  // ADDED at correct level
+     {
+       "name": "jsonwebtoken",
+       "version": "9.0.2",
+       "justification": "Industry standard JWT library",
+       "alternatives_considered": ["jose", "passport-jwt"],
+       "security_scan_status": "passed"
+     }
+   ],
    "tools_and_resources": [
      {
        "name": "Redis",
        "purpose": "Store refresh token blacklist",
-       "new_dependencies": [...]  // REMOVED from here
      }
    ]
  },
  "plan": {
    "phases": [...],
+   "decision_records": [  // NOW REQUIRED
+     {
+       "id": "ADR-001",
+       "title": "JWT vs Session Authentication",
+       "decision": "Use JWT with Redis refresh tokens",
+       "status": "accepted",
+       "rationale": "Enables stateless scaling",
+       "alternatives": ["Pure JWT", "Session-based"],
+       "consequences": { /* ... */ }
+     }
+   ]
  }
}
```

---

### Glossary

**ADR (Architecture Decision Record)**: Structured document capturing significant architectural or design decisions, including rationale, alternatives, and consequences.

**Agent**: An AI system capable of autonomous task execution within defined boundaries.

**Chain-of-Thought (CoT)**: Internal reasoning process (should NOT be output or logged; use Decision Records instead).

**Context Window**: Maximum number of tokens an LLM can process in a single request.

**Decision Record**: Auditable summary of a decision, alternatives considered, and rationale (replaces raw CoT in outputs).

**Epic**: Large body of work that can be broken down into multiple user stories.

**Idempotent**: Operation that produces the same result regardless of how many times it's executed.

**Plan Manifest**: Machine-readable JSON file containing complete project state and plan.

**Prompt Injection**: Security attack where malicious instructions are embedded in data processed by an LLM.

**RAG (Retrieval-Augmented Generation)**: Technique of augmenting LLM prompts with retrieved relevant documents.

**Tool Use / Function Calling**: LLM capability to invoke external functions or APIs.

---

### Further Reading

**AI Safety & Alignment**:
- [Anthropic: Constitutional AI](https://www.anthropic.com/index/constitutional-ai-harmlessness-from-ai-feedback)
- [OpenAI: Safety Best Practices](https://platform.openai.com/docs/guides/safety-best-practices)

**Prompt Engineering**:
- [Anthropic: Prompt Engineering Guide](https://docs.anthropic.com/claude/docs/prompt-engineering)
- [OpenAI: Best Practices for Prompt Engineering](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api)

**Software Engineering**:
- [Architectural Decision Records (ADRs)](https://adr.github.io/)
- [The Twelve-Factor App](https://12factor.net/)
- [Google SRE Book](https://sre.google/sre-book/table-of-contents/)

**Security**:
- [OWASP Top 10 for LLMs](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
- [NIST AI Risk Management Framework](https://www.nist.gov/itl/ai-risk-management-framework)

**Research Papers**:
- [Chain-of-Thought Prompting](https://arxiv.org/abs/2201.11903)
- [ReAct: Reasoning and Acting in LLMs](https://arxiv.org/abs/2210.03629)
- [Constitutional AI Paper](https://arxiv.org/abs/2212.08073)
- [Retrieval-Augmented Generation](https://arxiv.org/abs/2005.11401)

---

### Version History

**v2.2 (2025-11-11)** - Security & Cost-Optimized Edition
- ðŸ†• Added comprehensive Prompt Injection & Tool-Use Safety section
- ðŸ”„ Fixed schema/validator alignment (meta.prompt_version, dependencies.new_packages)
- ðŸ”„ Accurate cost modeling with separate first-call and cached-call functions
- ðŸ”„ Replaced public CoT output with Decision Records (ADR pattern)
- ðŸ”„ Enhanced reasoning privacy - never log raw internal reasoning
- ðŸ”„ CI/CD portability improvements (ecosystem-aware, conditional tooling)
- ðŸ”„ Added Determinism & Sampling guidance with standard configurations
- ðŸ”„ Parameterized models and pricing (no hard-coded values)
- âœ… Removed duplicate "Frontend UI Feature" section stub
- âœ… Enhanced observability section with privacy controls
- âœ… Updated all examples to use schema v2.2
- âœ… Comprehensive validation script updates

**v2.1 (2025-11-11)** - Merged Complete Edition
- Integrated: Complete enforcement validation script with CI/CD examples
- Integrated: Comprehensive cost modeling and budgeting framework
- Integrated: Four critical decision trees (agent selection, validation failure, rollback, context management)
- Integrated: One-page quick reference card for daily use
- Enhanced: Navigation with quick start section
- Enhanced: Organization and section numbering
- Enhanced: Production operations section

**v2.0 (2025-11-11)**
- Added: Chain-of-thought and reasoning patterns (CoT, Tree-of-Thoughts, Reflection)
- Added: Multi-agent orchestration strategies with communication protocols
- Added: RAG integration guidelines and patterns
- Added: Structured output modes (JSON mode, function calling)
- Added: Enhanced security and compliance sections with GDPR checklist
- Added: Comprehensive error handling and recovery patterns
- Added: Observability and metrics framework with dashboards
- Added: Prompt versioning and regression testing
- Added: Few-shot example patterns
- Added: Tool use / function calling integration
- Added: Specialized task patterns (Frontend, API, Data Pipeline, IaC, Refactoring, Bug Fix)
- Enhanced: JSON schema v2.0 with additional validation
- Enhanced: Context management strategies with token budgets
- Enhanced: Testing pyramid with property-based testing
- Enhanced: Dependency management protocol with supply chain security
- Improved: Template examples with before/after comparison
- Improved: Organization with comprehensive table of contents

**v1.0 (2025-10-01)**
- Initial release with core prompt architecture
- Basic planning and execution framework
- Essential security and compliance guidelines
- Fundamental testing requirements

---

## License

This guide is provided as-is for internal and external use. You may adapt, modify, and distribute it freely. Attribution appreciated but not required.

---

## Feedback & Support

**Document Version**: 2.2 (Security & Cost-Optimized Edition)
**Last Updated**: November 11, 2025  
**Maintainer**: [Your Team/Name]  
**Issues/Questions**: [GitHub Issues URL or email]  
**Discussions**: [Slack channel, Discord, or forum]

---

## Acknowledgments

This guide synthesizes best practices from:
- Software engineering (TDD, Clean Code, SOLID principles, ADRs)
- API design (OpenAPI, REST, GraphQL communities)
- AI research (Chain-of-Thought, Constitutional AI, RAG)
- AI safety (Prompt injection research, OWASP LLM Top 10)
- DevOps (SRE principles, incident response, monitoring)
- Security (OWASP, CWE, security engineering, secure development lifecycle)

Special thanks to practitioners who tested early versions and provided feedback, particularly the detailed v2.2 security and cost modeling improvements.

---

**End of Guide v2.2**

*This Security & Cost-Optimized Edition addresses critical production needs with comprehensive prompt injection defenses, accurate cost modeling, schema corrections, and enhanced privacy controls - everything you need for secure, cost-effective AI-assisted development.*
